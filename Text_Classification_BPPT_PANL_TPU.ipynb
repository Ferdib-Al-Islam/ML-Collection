{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - BPPT PANL - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "#!pip install -U ntlk\n",
        "#!pip install torch\n",
        "#import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e87253fb-d6a0-4c11-b8e4-fead4519a4bc"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "try:\n",
        "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "except KeyError:\n",
        "  TPU_WORKER = None\n",
        "\n",
        "np.random.seed(seed=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6f2255e6-4771-40f4-d728-d51babd7e4f0"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "88501656-1dd9-454d-d3ec-6990f69e4735"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 3.0G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  32M Oct 11 07:58 bard.h5\n",
            "drwx------ 2 root root 4.0K Oct  9 15:04 BPPTIndToEngCorpus\n",
            "-rw------- 1 root root  25M Oct  9 16:46 wiki.id.10K.vec\n",
            "-rw------- 1 root root 755M Oct  9 16:35 wiki.id.300K.vec\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n",
            "total 9.2M\n",
            "-rw------- 1 root root 4.6M Oct  9 15:04 bppt_panl.csv\n",
            "-rw------- 1 root root 476K Oct  9 15:04 bppt_panl_test.csv\n",
            "-rw------- 1 root root 4.2M Oct  9 15:04 bppt_panl_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7JIIAgpafrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
        "test_df.columns = ['label', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r4rofDLbCPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4d4d1d22-e8a7-41a7-8ee7-8e0f9ca8b6b3"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "#print(train_df['label'][:10].values)\n",
        "#print(train_df['text'][:10].values)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      3  Spurs menguasai permainan sejak saat Keane men...\n",
            "1      0  Saat ini juga sedang berlangsung negosiasi den...\n",
            "2      3  Pemain tengah Fenerbahce, Deivid, beralih dari...\n",
            "3      0  Menurut Kepala Perwakilan BPKP di Provinsi Pap...\n",
            "4      0  Indonesia memiliki potensi yang luar biasa seb...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "96f713b7-3f14-417e-9e47-10dd834ceb76"
      },
      "cell_type": "code",
      "source": [
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COLAB_TPU_ADDR=10.3.206.250:8470\n",
            "DATALAB_SETTINGS_OVERRIDES='{\"datalabBasePath\":\"/tun/m/tpu-2769hgji40dub/\",\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"notebook\",\"-y\",\"--no-browser\",\"--log-level=DEBUG\",\"--debug\",\"--NotebookApp.allow_origin=\\\"*\\\"\",\"--NotebookApp.log_format=\\\"%(message)s\\\"\",\"--NotebookApp.disable_check_xsrf=True\",\"--NotebookApp.token=\",\"--Session.key=\\\"\\\"\",\"--Session.keyfile=\\\"\\\"\",\"--ContentsManager.untitled_directory=\\\"Untitled Folder\\\"\",\"--ContentsManager.untitled_file=\\\"Untitled File\\\"\",\"--ContentsManager.untitled_notebook=\\\"Untitled Notebook\\\"\",\"--KernelManager.autorestart=True\",\"--ip=\\\"172.28.0.2\\\"\"]}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "93c11bcb-fb3d-4147-ff42-a1bb4f1d9d15"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19469    Pada petenis unggulan mendapatkan bye pada per...\n",
            "24559    Pencetak gol terbanyak Serie A David Trezeguet...\n",
            "5322     PT Pemeringkat Efek Indonesia Pefindo memberik...\n",
            "15053    Gas untuk keperluan rumah tangga misalnya, mes...\n",
            "20938    Kapten Bayern Munich Oliver Kahn memperingatka...\n",
            "Name: text, dtype: object\n",
            "[3 3 0 0 3]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "Name: text, dtype: object\n",
            "[1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_df['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfzSkzK9OSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(xtrain_count, xvalid_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(train_df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1657c63-da1f-4bec-b71a-441b49cf6380"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki.id.10K.vec')):\n",
        "  if i%10000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
        "  except ValueError:\n",
        "    print(\"Values: {}: {}\".format(i, values))\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train_df['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\" predictions:\", predictions[:20])\n",
        "    print(\"ground truth:\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5a21a7af-9ade-4fbf-81d8-2388f8d582dd"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, Count Vectors:  0.9360279150915964\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, WordLevel TF-IDF:  0.9274498400697877\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 1 1 0 0 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, N-Gram Vectors:  0.7916545507414946\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, CharLevel Vectors:  0.8685664437336436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7187b85b-5472-4cd5-9f8d-14477f3bb421"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, Count Vectors:  0.9520209363186973\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, WordLevel TF-IDF:  0.9358825239895319\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 1 3 0 3 0 1 0 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, N-Gram Vectors:  0.8269845885431811\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, CharLevel Vectors:  0.9017156150043617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "500adb3a-84bd-4ee2-dddb-4d341a38b962"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "SVM, N-Gram Vectors:  0.4287583599883687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3dfa214e-13af-404a-84ee-bf289eab2ce1"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 0 2 0 3 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, Count Vectors:  0.8871765047979063\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 2 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, WordLevel TF-IDF:  0.883396336144228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8583a3df-3a8f-44df-e00a-518e5c32a17a"
      },
      "cell_type": "code",
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, Count Vectors:  0.8200058156440826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 0 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, WordLevel TF-IDF:  0.8220412910729863\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, CharLevel Vectors:  0.8298924105844723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOTyMac-ScU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-ZDXV--SySi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tpu_wrapper(func):\n",
        "  if TPU_WORKER is not None:\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    return tpu_model\n",
        "  else:\n",
        "    return func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This model doesn't work with current dataset\n",
        "def create_simple_model(input_length=100):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_length, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(4, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "#classifier = create_simple_model(xtrain_tfidf_ngram.shape[1])\n",
        "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "#                       is_neural_net=True, epochs=1)\n",
        "#print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI6s_qBsSjzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "#NN, Ngram Level TF IDF Vectors\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_simple_model(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53e005d3-133b-4331-869d-1b1e1cbb161c"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20631, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def to_tpu(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "      print(\"TPU Wrapper start\")\n",
        "      print(func)\n",
        "      if TPU_WORKER is not None:\n",
        "        tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "        print(\"TPU exist\")\n",
        "        return tpu_model(*args, **kwargs)\n",
        "      else:\n",
        "        print(\"TPU not exist\")\n",
        "        return func(*args, **kwargs)\n",
        "    print(\"TO_TPU\")\n",
        "    return wrapper\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOK2hrFuYnIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1040
        },
        "outputId": "34a2f841-57e9-46a8-b137-793467de3f50"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: b'grpc://10.3.206.250:8470'\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.7215137481689453 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.9698 - acc: 0.6209INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 2.9164888858795166 secs\n",
            "20/20 [==============================] - 13s 642ms/step - loss: 0.9519 - acc: 0.6289 - val_loss: 0.4711 - val_acc: 0.8711\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.4277 - acc: 0.8562 - val_loss: 0.3003 - val_acc: 0.8958\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 3s 133ms/step - loss: 0.3446 - acc: 0.8824 - val_loss: 0.2777 - val_acc: 0.9010\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 3s 140ms/step - loss: 0.3258 - acc: 0.8797 - val_loss: 0.2595 - val_acc: 0.9076\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.2952 - acc: 0.8965 - val_loss: 0.2376 - val_acc: 0.9167\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 3s 139ms/step - loss: 0.2527 - acc: 0.9137 - val_loss: 0.2298 - val_acc: 0.9154\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.2367 - acc: 0.9152 - val_loss: 0.2224 - val_acc: 0.9258\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 3s 141ms/step - loss: 0.2316 - acc: 0.9160 - val_loss: 0.2119 - val_acc: 0.9297\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.2181 - acc: 0.9273 - val_loss: 0.2173 - val_acc: 0.9310\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.2050 - acc: 0.9301 - val_loss: 0.1940 - val_acc: 0.9297\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 0.1731 - acc: 0.9422 - val_loss: 0.2029 - val_acc: 0.9336\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.1600 - acc: 0.9457 - val_loss: 0.1766 - val_acc: 0.9349\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.1665 - acc: 0.9426 - val_loss: 0.1653 - val_acc: 0.9440\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.1359 - acc: 0.9516 - val_loss: 0.1645 - val_acc: 0.9427\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1485 - acc: 0.9445 - val_loss: 0.1996 - val_acc: 0.9323\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.1134 - acc: 0.9598 - val_loss: 0.1824 - val_acc: 0.9336\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.1181 - acc: 0.9609 - val_loss: 0.2134 - val_acc: 0.9271\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.1173 - acc: 0.9586 - val_loss: 0.2089 - val_acc: 0.9336\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.0931 - acc: 0.9691 - val_loss: 0.2112 - val_acc: 0.9362\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.0918 - acc: 0.9668 - val_loss: 0.1837 - val_acc: 0.9414\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "CPU times: user 38.1 s, sys: 7.8 s, total: 45.9 s\n",
            "Wall time: 1min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0afb27ed-e6b8-4efb-dc50-5ec6271a972a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bed7128d-214b-4114-a5dc-0d5392b84600"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on test dataset\n",
        "\n",
        "\n",
        "sequences = token.texts_to_sequences(test_df['text'].values)\n",
        "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "# TPU can be enabled here if we need  \n",
        "# classifier = tpu_wrapper(classifier)\n",
        "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
        "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 1s 487us/step\n",
            "Loss for final step: 0.18855890020221003, accuracy: 0.9355366492146597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "diSp_8Sg9rrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Just for testing\n",
        "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
        "#print(labels[:5])\n",
        "print(np.array(labels)[:5])\n",
        "print(test_df['label'].values)\n",
        "print(to_categorical(3, 4))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7bb7bc1a-7159-4000-8846-fe27846dd539"
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(test_x_seq, verbose=1)\n",
        "#print(prediction)\n",
        "print('Predictions for final step: {}'.format(np.argmax(score, axis=1)[:10]))\n",
        "print(valid_x[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 2s 601us/step\n",
            "Predictions for final step: [2 1 0 0 0 1 2 0 2 0]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "19559    Agri Resources BV, sebuah perusahaan di Beland...\n",
            "6336     Asalkan kita punya program-program yang bagus ...\n",
            "2881     Sikap ini diperkuat oleh hasil interogasi terh...\n",
            "16877            Gurita juga menggunakan sistem yang sama.\n",
            "26868    Tantangan utamanya mungkin akan datang dari Ra...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2af27a45-888b-4b0b-d0a0-468c006d8dfa"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335212     1220196     8944632         804     3170384    12348052\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2d75fdf6-1810-4580-9b59-856c8a7e909c"
      },
      "cell_type": "code",
      "source": [
        "print(score)\n",
        "print(np.argmax(score, axis=1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.6630200e-04 1.0124649e-03 9.9810225e-01 1.8999004e-05]\n",
            " [3.0715969e-06 9.9983561e-01 1.4403675e-04 1.7290302e-05]\n",
            " [9.5167154e-01 1.1388691e-02 3.6110844e-02 8.2881597e-04]\n",
            " ...\n",
            " [1.1887937e-03 1.5343540e-02 9.8264515e-01 8.2238560e-04]\n",
            " [9.9969316e-01 4.9298887e-07 3.0627038e-04 8.0982971e-08]\n",
            " [7.7077589e-04 3.7112991e-06 9.9922478e-01 7.3053889e-07]]\n",
            "[2 1 0 ... 2 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
        "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
        "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a1069dec-8f3c-456c-f67f-879107269a0b"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# Speed comparison between CPU vs TPU\n",
        "# First we test CPU with 2 epochs\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "20/20 [==============================] - 79s 4s/step - loss: 1.1150 - acc: 0.5111 - val_loss: 0.6440 - val_acc: 0.8319\n",
            "Epoch 2/2\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.5015 - acc: 0.8368 - val_loss: 0.3377 - val_acc: 0.8830\n",
            "CPU times: user 4min 50s, sys: 10.2 s, total: 5min\n",
            "Wall time: 2min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MuKd3kFMpzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "8115eccc-e88f-40ce-e066-2eafdbd46d8b"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# We test now the TPU\n",
        "# The result is:\n",
        "# CPU: 77s/epoch\n",
        "# TPU: 3.15s/epoch\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_6/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.248517751693726 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 1.0701 - acc: 0.5391INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.812144994735718 secs\n",
            "20/20 [==============================] - 19s 952ms/step - loss: 1.0501 - acc: 0.5535 - val_loss: 0.5264 - val_acc: 0.8464\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.4643 - acc: 0.8465 - val_loss: 0.3696 - val_acc: 0.8607\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 3s 151ms/step - loss: 0.3061 - acc: 0.8973 - val_loss: 0.3387 - val_acc: 0.8763\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 3s 147ms/step - loss: 0.3307 - acc: 0.8750 - val_loss: 0.2515 - val_acc: 0.9180\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 3s 162ms/step - loss: 0.3005 - acc: 0.8992 - val_loss: 0.2628 - val_acc: 0.9128\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 3s 160ms/step - loss: 0.2917 - acc: 0.8906 - val_loss: 0.2978 - val_acc: 0.8919\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.2864 - acc: 0.8961 - val_loss: 0.2607 - val_acc: 0.9180\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2628 - acc: 0.9102 - val_loss: 0.2270 - val_acc: 0.9245\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 3s 151ms/step - loss: 0.2507 - acc: 0.9102 - val_loss: 0.2439 - val_acc: 0.9219\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 3s 147ms/step - loss: 0.2598 - acc: 0.9000 - val_loss: 0.2436 - val_acc: 0.9245\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.2312 - acc: 0.9219 - val_loss: 0.2478 - val_acc: 0.9128\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.2524 - acc: 0.9082 - val_loss: 0.1991 - val_acc: 0.9180\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2245 - acc: 0.9250 - val_loss: 0.2417 - val_acc: 0.9128\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 156ms/step - loss: 0.2133 - acc: 0.9258 - val_loss: 0.3700 - val_acc: 0.8724\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 3s 148ms/step - loss: 0.2039 - acc: 0.9289 - val_loss: 0.2236 - val_acc: 0.9193\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 4s 175ms/step - loss: 0.2144 - acc: 0.9184 - val_loss: 0.2357 - val_acc: 0.9180\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.2100 - acc: 0.9258 - val_loss: 0.1859 - val_acc: 0.9258\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.2203 - acc: 0.9289 - val_loss: 0.1699 - val_acc: 0.9414\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 155ms/step - loss: 0.2120 - acc: 0.9219 - val_loss: 0.2602 - val_acc: 0.9089\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.2138 - acc: 0.9277 - val_loss: 0.2180 - val_acc: 0.9271\n",
            "CPU times: user 39.6 s, sys: 7.2 s, total: 46.8 s\n",
            "Wall time: 1min 18s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_gru(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NP0Ea8yQNprO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "e56c9764-5fcf-4edf-b890-1f66c5459889"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-GRU, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_gru(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_8/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.248059988021851 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 1.1141 - acc: 0.5259INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.8303563594818115 secs\n",
            "20/20 [==============================] - 20s 1s/step - loss: 1.0953 - acc: 0.5363 - val_loss: 0.7343 - val_acc: 0.7617\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.5933 - acc: 0.8004 - val_loss: 0.3466 - val_acc: 0.8750\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 3s 140ms/step - loss: 0.4196 - acc: 0.8430 - val_loss: 0.3227 - val_acc: 0.8841\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.3458 - acc: 0.8777 - val_loss: 0.2661 - val_acc: 0.9089\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2975 - acc: 0.9016 - val_loss: 0.2409 - val_acc: 0.9206\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.2943 - acc: 0.8957 - val_loss: 0.2377 - val_acc: 0.9049\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 3s 151ms/step - loss: 0.2665 - acc: 0.9098 - val_loss: 0.2401 - val_acc: 0.9128\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.2852 - acc: 0.9020 - val_loss: 0.2770 - val_acc: 0.8997\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.2822 - acc: 0.8984 - val_loss: 0.2344 - val_acc: 0.9193\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 3s 157ms/step - loss: 0.2552 - acc: 0.9137 - val_loss: 0.2100 - val_acc: 0.9245\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.2650 - acc: 0.9074 - val_loss: 0.2117 - val_acc: 0.9219\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.2370 - acc: 0.9109 - val_loss: 0.2526 - val_acc: 0.9115\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2340 - acc: 0.9199 - val_loss: 0.2632 - val_acc: 0.9154\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2189 - acc: 0.9203 - val_loss: 0.2147 - val_acc: 0.9284\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 3s 147ms/step - loss: 0.2120 - acc: 0.9242 - val_loss: 0.2357 - val_acc: 0.9180\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2214 - acc: 0.9270 - val_loss: 0.2191 - val_acc: 0.9232\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 3s 148ms/step - loss: 0.2027 - acc: 0.9305 - val_loss: 0.2425 - val_acc: 0.9154\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 3s 151ms/step - loss: 0.2159 - acc: 0.9289 - val_loss: 0.1951 - val_acc: 0.9323\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.2051 - acc: 0.9297 - val_loss: 0.2064 - val_acc: 0.9245\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.1804 - acc: 0.9391 - val_loss: 0.1660 - val_acc: 0.9440\n",
            "CPU times: user 40.1 s, sys: 7.3 s, total: 47.4 s\n",
            "Wall time: 1min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RNN-Bidirectional, Word Embeddings\n",
        "# It doesn't work with TPU, but it works on CPU/GPU\n",
        "def create_bidirectional_rnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyS7YPyWOiD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "7965aef1-4b66-46fd-e0ec-89d70e1b8158"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-Bidirectional, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 19s 973ms/step - loss: 1.2091 - acc: 0.4466 - val_loss: 0.9484 - val_acc: 0.6694\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 14s 717ms/step - loss: 0.7157 - acc: 0.7386 - val_loss: 0.4386 - val_acc: 0.8530\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 738ms/step - loss: 0.4169 - acc: 0.8549 - val_loss: 0.3147 - val_acc: 0.8856\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 14s 711ms/step - loss: 0.3404 - acc: 0.8826 - val_loss: 0.2709 - val_acc: 0.9048\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 15s 755ms/step - loss: 0.3073 - acc: 0.8938 - val_loss: 0.2566 - val_acc: 0.9069\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 733ms/step - loss: 0.2938 - acc: 0.8962 - val_loss: 0.2484 - val_acc: 0.9116\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 15s 751ms/step - loss: 0.2748 - acc: 0.9028 - val_loss: 0.2504 - val_acc: 0.9118\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 15s 734ms/step - loss: 0.2702 - acc: 0.9079 - val_loss: 0.2405 - val_acc: 0.9139\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 741ms/step - loss: 0.2642 - acc: 0.9078 - val_loss: 0.2345 - val_acc: 0.9147\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 15s 738ms/step - loss: 0.2549 - acc: 0.9114 - val_loss: 0.2277 - val_acc: 0.9183\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 743ms/step - loss: 0.2463 - acc: 0.9142 - val_loss: 0.2253 - val_acc: 0.9199\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 741ms/step - loss: 0.2397 - acc: 0.9161 - val_loss: 0.2223 - val_acc: 0.9207\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 15s 727ms/step - loss: 0.2312 - acc: 0.9181 - val_loss: 0.2198 - val_acc: 0.9237\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 15s 756ms/step - loss: 0.2261 - acc: 0.9220 - val_loss: 0.2128 - val_acc: 0.9250\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 15s 744ms/step - loss: 0.2209 - acc: 0.9232 - val_loss: 0.2108 - val_acc: 0.9261\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 758ms/step - loss: 0.2149 - acc: 0.9266 - val_loss: 0.2160 - val_acc: 0.9230\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 698ms/step - loss: 0.2118 - acc: 0.9253 - val_loss: 0.2190 - val_acc: 0.9237\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 15s 746ms/step - loss: 0.2095 - acc: 0.9269 - val_loss: 0.2134 - val_acc: 0.9255\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 704ms/step - loss: 0.2032 - acc: 0.9291 - val_loss: 0.2007 - val_acc: 0.9292\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 15s 747ms/step - loss: 0.1949 - acc: 0.9321 - val_loss: 0.2041 - val_acc: 0.9308\n",
            "CPU times: user 7min 55s, sys: 1min, total: 8min 55s\n",
            "Wall time: 5min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rcnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "#classifier = create_rcnn()\n",
        "#accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "#                       is_neural_net=True, epochs=10)\n",
        "#print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "bec35936-82a9-4d42-c8dc-9fbed1435687"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RCNN, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 1.1720 - acc: 0.4778 - val_loss: 0.8795 - val_acc: 0.6781\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 15s 737ms/step - loss: 0.6697 - acc: 0.7627 - val_loss: 0.3682 - val_acc: 0.8750\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 767ms/step - loss: 0.3951 - acc: 0.8652 - val_loss: 0.2989 - val_acc: 0.8958\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.3331 - acc: 0.8842 - val_loss: 0.2719 - val_acc: 0.9027\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.3086 - acc: 0.8920 - val_loss: 0.2577 - val_acc: 0.9085\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 726ms/step - loss: 0.2913 - acc: 0.8978 - val_loss: 0.2602 - val_acc: 0.9106\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.2788 - acc: 0.9010 - val_loss: 0.2493 - val_acc: 0.9116\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 14s 718ms/step - loss: 0.2728 - acc: 0.9040 - val_loss: 0.2403 - val_acc: 0.9160\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 752ms/step - loss: 0.2595 - acc: 0.9107 - val_loss: 0.2350 - val_acc: 0.9162\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 15s 738ms/step - loss: 0.2504 - acc: 0.9119 - val_loss: 0.2329 - val_acc: 0.9191\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 742ms/step - loss: 0.2422 - acc: 0.9171 - val_loss: 0.2354 - val_acc: 0.9194\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 743ms/step - loss: 0.2377 - acc: 0.9157 - val_loss: 0.2263 - val_acc: 0.9201\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 15s 728ms/step - loss: 0.2304 - acc: 0.9206 - val_loss: 0.2275 - val_acc: 0.9212\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 15s 741ms/step - loss: 0.2250 - acc: 0.9222 - val_loss: 0.2191 - val_acc: 0.9201\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 14s 721ms/step - loss: 0.2167 - acc: 0.9246 - val_loss: 0.2145 - val_acc: 0.9230\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 745ms/step - loss: 0.2133 - acc: 0.9239 - val_loss: 0.2116 - val_acc: 0.9251\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.2073 - acc: 0.9272 - val_loss: 0.2179 - val_acc: 0.9240\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.2040 - acc: 0.9280 - val_loss: 0.2116 - val_acc: 0.9285\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 0.2000 - acc: 0.9299 - val_loss: 0.2085 - val_acc: 0.9282\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.1947 - acc: 0.9312 - val_loss: 0.2079 - val_acc: 0.9284\n",
            "CPU times: user 7min 58s, sys: 59.2 s, total: 8min 57s\n",
            "Wall time: 5min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}