{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - BPPT PANL - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "#!pip install -U ntlk\n",
        "#!pip install torch\n",
        "#import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "try:\n",
        "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "except KeyError:\n",
        "  TPU_WORKER = None\n",
        "\n",
        "np.random.seed(seed=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "57f21b79-467b-4136-8dad-fa118b34e0be"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 3.0G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  31M Oct 11 13:37 bard.h5\n",
            "drwx------ 2 root root 4.0K Oct  9 15:04 BPPTIndToEngCorpus\n",
            "-rw------- 1 root root  25M Oct  9 16:46 wiki.id.10K.vec\n",
            "-rw------- 1 root root 755M Oct  9 16:35 wiki.id.300K.vec\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n",
            "total 7.1M\n",
            "-rw------- 1 root root 3.6M Oct 11 10:45 bppt_panl.csv\n",
            "-rw------- 1 root root 362K Oct 11 10:45 bppt_panl_test.csv\n",
            "-rw------- 1 root root 3.2M Oct 11 10:45 bppt_panl_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7JIIAgpafrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We use dataset from:\n",
        "# http://www.panl10n.net/english/outputs/Indonesia/BPPT/0902/BPPTIndToEngCorpusHalfM.zip\n",
        "# It is \"500,000 Word Bahasa Indonesia Corpus and Parallel English Translation\"\n",
        "# and contains:\n",
        "# - 500,000 words from the Penn Treebank Corpus manually translated into Bahasa Indonesian, \n",
        "# - 500,000 words from various online sources translated into English.\n",
        "# For our text classification, we use only the indonesian part.\n",
        "# The corpus has 4 classes:\n",
        "# - 0: Economy\n",
        "# - 1: International\n",
        "# - 2: Science\n",
        "# - 3: Sport\n",
        "# Originally each class is in separate file, we combine, randomize and split \n",
        "# it to train and test file with 90:10.  \n",
        "\n",
        "\n",
        "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
        "test_df.columns = ['label', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r4rofDLbCPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8cf1a6bb-179f-47ed-b50a-e4760706406b"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "#print(train_df['label'][:10].values)\n",
        "#print(train_df['text'][:10].values)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      0  Pertumbuhan ekonomi 2007 yang diproyeksikan me...\n",
            "1      3  Pelatih Real Bernd Schuster harus mengeluarkan...\n",
            "2      2  Laporan itu adalah pengumuman kedua dari badan...\n",
            "3      0  Lonjakan laba bersih tersebut, selain didorong...\n",
            "4      3  LeBron James menyumbang 24 poin, 11 assist dan...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5c95a3a9-0a9d-4463-b2cb-c52d5fd563e2"
      },
      "cell_type": "code",
      "source": [
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COLAB_TPU_ADDR=10.83.101.58:8470\n",
            "DATALAB_SETTINGS_OVERRIDES='{\"datalabBasePath\":\"/tun/m/tpu-g0mblftse7m0/\",\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"notebook\",\"-y\",\"--no-browser\",\"--log-level=DEBUG\",\"--debug\",\"--NotebookApp.allow_origin=\\\"*\\\"\",\"--NotebookApp.log_format=\\\"%(message)s\\\"\",\"--NotebookApp.disable_check_xsrf=True\",\"--NotebookApp.token=\",\"--Session.key=\\\"\\\"\",\"--Session.keyfile=\\\"\\\"\",\"--ContentsManager.untitled_directory=\\\"Untitled Folder\\\"\",\"--ContentsManager.untitled_file=\\\"Untitled File\\\"\",\"--ContentsManager.untitled_notebook=\\\"Untitled Notebook\\\"\",\"--KernelManager.autorestart=True\",\"--ip=\\\"172.28.0.2\\\"\"]}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f6d92597-1b7c-442d-f301-915ddbdb25ca"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19555    Layanan fantastis ini membutuhkan waktu lama u...\n",
            "9774     Gasparotto memimpin lomba keseluruhan disusul ...\n",
            "10459    Pereli Prancis itu unggul dua menit 33,2 detik...\n",
            "19916    Karena itu Antam juga melakukan diversifikasi ...\n",
            "15343    Para kapitalis ini menginginkan pemerintah Ind...\n",
            "Name: text, dtype: object\n",
            "[2 3 3 0 0]\n",
            "3081     Menurut Shahab, peralatan Rig pada BJP-1R1 sud...\n",
            "1601     Anak pepsis memakan daging tarantula dan berli...\n",
            "20221    Dalam hal kepemilikan asset, maka sektor-sekto...\n",
            "2118     Ilmuwan telah menemukan informasi genetik yang...\n",
            "17469    Anda menyesuaikan dengan itu dan lawan juga sama.\n",
            "Name: text, dtype: object\n",
            "[0 2 0 2 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_df['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(train_df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9de91357-2191-4368-f7d2-06c883ba4fec"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki.id.10K.vec')):\n",
        "  if i%10000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
        "  except ValueError:\n",
        "    print(\"Values: {}: {}\".format(i, values))\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train_df['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\" predictions:\", predictions[:20])\n",
        "    print(\"ground truth:\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dc7d34ad-c988-43da-a70a-f943d55e7f0d"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 3 0 0 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, Count Vectors:  0.9269195189639223\n",
            " predictions: [0 2 0 2 3 0 1 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, WordLevel TF-IDF:  0.9161887141535615\n",
            " predictions: [0 2 0 2 1 0 1 2 0 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, N-Gram Vectors:  0.7822386679000926\n",
            " predictions: [0 2 0 2 2 0 0 2 2 1 2 2 0 0 1 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, CharLevel Vectors:  0.8432932469935245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cf3485b2-d99c-42df-90a9-a9ed36fbbbae"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 2 0 0 2 2 1 0 2 2 0 0 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, Count Vectors:  0.9265494912118409\n",
            " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, WordLevel TF-IDF:  0.9178538390379278\n",
            " predictions: [0 2 2 2 1 0 1 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, N-Gram Vectors:  0.8085106382978723\n",
            " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 2 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, CharLevel Vectors:  0.8888066604995375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a07d0c16-c7cf-4b38-edf0-edb874626bb8"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "# It seems there is something wrong here, since the predictions are always 0\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "SVM, N-Gram Vectors:  0.25827937095282144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5e38f57c-1f52-404c-e4b2-7a3a374d20ec"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 2 0 0 2 0 1 0 2 0 0 0 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "RF, Count Vectors:  0.8390379278445883\n",
            " predictions: [0 2 0 2 1 0 0 2 0 2 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "RF, WordLevel TF-IDF:  0.8325624421831638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "aa14b4c4-48c1-4d75-b4fc-83c85c164c59"
      },
      "cell_type": "code",
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, Count Vectors:  0.808695652173913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, WordLevel TF-IDF:  0.8070305272895467\n",
            " predictions: [0 2 0 2 2 0 1 2 2 2 0 2 2 0 1 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, CharLevel Vectors:  0.8201665124884366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOTyMac-ScU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-ZDXV--SySi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tpu_wrapper(func):\n",
        "  if TPU_WORKER is not None:\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    return tpu_model\n",
        "  else:\n",
        "    return func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This model doesn't work with current dataset\n",
        "def create_simple_model(input_length=100):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_length, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(4, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "#classifier = create_simple_model(xtrain_tfidf_ngram.shape[1])\n",
        "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "#                       is_neural_net=True, epochs=1)\n",
        "#print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI6s_qBsSjzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "#NN, Ngram Level TF IDF Vectors\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_simple_model(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e6788c1-ee70-4e55-9081-43c99d39a967"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16214, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def to_tpu(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "      print(\"TPU Wrapper start\")\n",
        "      print(func)\n",
        "      if TPU_WORKER is not None:\n",
        "        tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "        print(\"TPU exist\")\n",
        "        return tpu_model(*args, **kwargs)\n",
        "      else:\n",
        "        print(\"TPU not exist\")\n",
        "        return func(*args, **kwargs)\n",
        "    print(\"TO_TPU\")\n",
        "    return wrapper\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1040
        },
        "outputId": "d0f21702-a259-4014-8c46-15ee51bf1994"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: b'grpc://10.83.101.58:8470'\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.4450600147247314 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.1848 - acc: 0.4933INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 2.99385666847229 secs\n",
            "15/15 [==============================] - 12s 812ms/step - loss: 1.1638 - acc: 0.5094 - val_loss: 0.7591 - val_acc: 0.8266\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 2s 142ms/step - loss: 0.6758 - acc: 0.7802 - val_loss: 0.4231 - val_acc: 0.8453\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 2s 158ms/step - loss: 0.4297 - acc: 0.8599 - val_loss: 0.3853 - val_acc: 0.8609\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 2s 147ms/step - loss: 0.3788 - acc: 0.8661 - val_loss: 0.3555 - val_acc: 0.8750\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 2s 144ms/step - loss: 0.3396 - acc: 0.8839 - val_loss: 0.2679 - val_acc: 0.9062\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 2s 140ms/step - loss: 0.3258 - acc: 0.8958 - val_loss: 0.2498 - val_acc: 0.9031\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 2s 137ms/step - loss: 0.2986 - acc: 0.8948 - val_loss: 0.2632 - val_acc: 0.9031\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 2s 148ms/step - loss: 0.2846 - acc: 0.8943 - val_loss: 0.2575 - val_acc: 0.9062\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 2s 147ms/step - loss: 0.2649 - acc: 0.9052 - val_loss: 0.2301 - val_acc: 0.9172\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 2s 140ms/step - loss: 0.2396 - acc: 0.9167 - val_loss: 0.2880 - val_acc: 0.8891\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 2s 157ms/step - loss: 0.2327 - acc: 0.9187 - val_loss: 0.2948 - val_acc: 0.8922\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 2s 135ms/step - loss: 0.2431 - acc: 0.9193 - val_loss: 0.2384 - val_acc: 0.9156\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 2s 142ms/step - loss: 0.1973 - acc: 0.9281 - val_loss: 0.2560 - val_acc: 0.9000\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 2s 142ms/step - loss: 0.2074 - acc: 0.9234 - val_loss: 0.2557 - val_acc: 0.9062\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 2s 139ms/step - loss: 0.1788 - acc: 0.9354 - val_loss: 0.2197 - val_acc: 0.9313\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 2s 165ms/step - loss: 0.1880 - acc: 0.9354 - val_loss: 0.2479 - val_acc: 0.9141\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 2s 132ms/step - loss: 0.1633 - acc: 0.9448 - val_loss: 0.2162 - val_acc: 0.9156\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 2s 136ms/step - loss: 0.1649 - acc: 0.9406 - val_loss: 0.2171 - val_acc: 0.9125\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 2s 142ms/step - loss: 0.1769 - acc: 0.9417 - val_loss: 0.2262 - val_acc: 0.9094\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 2s 153ms/step - loss: 0.1542 - acc: 0.9443 - val_loss: 0.2114 - val_acc: 0.9203\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "CPU times: user 33.4 s, sys: 6.24 s, total: 39.6 s\n",
            "Wall time: 1min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0afb27ed-e6b8-4efb-dc50-5ec6271a972a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db3a4519-d56d-4419-d2aa-0127a44c3510"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on test dataset\n",
        "\n",
        "test_x = test_df['text'].values\n",
        "sequences = token.texts_to_sequences(test_x)\n",
        "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "# TPU can be enabled here if we need  \n",
        "# classifier = tpu_wrapper(classifier)\n",
        "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
        "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2402/2402 [==============================] - 2s 646us/step\n",
            "Loss for final step: 0.2228940686863924, accuracy: 0.9196502914238135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "diSp_8Sg9rrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Just for testing\n",
        "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
        "#print(labels[:5])\n",
        "print(np.array(labels)[:5])\n",
        "print(test_df['label'].values)\n",
        "print(to_categorical(3, 4))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0c071a4f-ff9f-4848-8a33-ad5e145c905b"
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(test_x_seq, verbose=1)\n",
        "#print(prediction)\n",
        "print('Predictions for final step: {}'.format(np.argmax(prediction, axis=1)[:10]))\n",
        "print(test_x[:10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2402/2402 [==============================] - 2s 631us/step\n",
            "Predictions for final step: [0 0 2 3 1 0 0 1 2 2]\n",
            "['Paradoksnya di sisi lain, sinyal akan diakuinya keberadaan lembaga keuangan mikro non formal juga membuat kegelisahan bagi pelakunya.'\n",
            " 'Menurut dia, harga minyak mentah dunia saat ini berada di posisi 126 dolar AS per barel sedikit melemah dibanding hari sebelumnya yang mencapai 127 dolar AS lebih.'\n",
            " 'Pengkajian ini memberikan konfirmasi bahwa kita menghadapi sejenis materi yang berbeda sama sekali, tak seperti yang kita bayangkan.'\n",
            " 'Gol akhir Marco Borriello membuat Genoa mendapat satu angka setelah striker asal Honduras, David Suazo, mencetak angka pada menit ke-12 untuk Inter, yang pemain tengahnya dari Portugal, Pele, dikeluarkan dari lapangan karena mendapat kartu kuning kedua sebelum turun minum.'\n",
            " 'Namun, bandar udara itu ditutup sebagai langkah pencegahan.'\n",
            " 'Departemen Keuangan telah menetapkan 16 calon agen penjual obligasi negara ritel ORI pada 2007 melalui SK Dirjen Pengelolaan Utang Depkeu No KEP-07/PU/2007 tertanggal 19 Februari 2007.'\n",
            " 'Kontrak kerja sama yang tekah ditandatangani dengan China mencapai puluhan juta dollar AS.'\n",
            " 'Sebab kejadian tersebut masih dalam penyelidikan, kata Letnan Kolonel Josslyn Aberle di Bagdad.'\n",
            " 'Kisah Di Balik Suatu Komunikasi Singkat.'\n",
            " 'Proses degradasi material organik ini tanpa melibatkan oksigen disebut anaerobik digestion Gas yang dihasilkan sebagian besar berupa metana.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d90d8629-1eff-4fcf-cc74-4cabe26d12ae"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335204     1260560     8939392         804     3135252    12365508\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5f212213-4fc3-4501-baa2-4be082e584f8"
      },
      "cell_type": "code",
      "source": [
        "print(prediction)\n",
        "print(np.argmax(prediction, axis=1))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.9899882e-01 4.6373406e-04 5.3280470e-04 4.7004282e-06]\n",
            " [9.9977285e-01 1.3875878e-04 7.5860429e-05 1.2497356e-05]\n",
            " [1.0611779e-01 4.8861925e-02 8.3545119e-01 9.5691448e-03]\n",
            " ...\n",
            " [1.1416495e-02 9.8778975e-01 4.8050229e-04 3.1316865e-04]\n",
            " [1.4843496e-03 5.0487202e-02 4.8084562e-03 9.4321996e-01]\n",
            " [1.4797889e-02 2.0483567e-03 9.8280776e-01 3.4594335e-04]]\n",
            "[0 0 2 ... 1 3 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
        "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
        "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "83f81f15-5d01-4dff-ac0c-50ccbb0470e7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# Speed comparison between CPU vs TPU\n",
        "# First we test CPU with 2 epochs\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=2\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "15/15 [==============================] - 65s 4s/step - loss: 1.0931 - acc: 0.6021 - val_loss: 0.5449 - val_acc: 0.8283\n",
            "Epoch 2/2\n",
            "15/15 [==============================] - 61s 4s/step - loss: 0.4863 - acc: 0.8357 - val_loss: 0.3735 - val_acc: 0.8699\n",
            "CPU times: user 3min 50s, sys: 10.1 s, total: 4min\n",
            "Wall time: 2min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MuKd3kFMpzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "2da78524-000f-4331-e377-c7cd318958b0"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# We test now the TPU\n",
        "# The result is:\n",
        "# CPU: 65s/epoch\n",
        "# TPU: 3.15s/epoch\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_2/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.745240688323975 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.1647 - acc: 0.5430INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.64632248878479 secs\n",
            "15/15 [==============================] - 16s 1s/step - loss: 1.1357 - acc: 0.5594 - val_loss: 0.6171 - val_acc: 0.8156\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 3s 167ms/step - loss: 0.5278 - acc: 0.8151 - val_loss: 0.3705 - val_acc: 0.8781\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 2s 152ms/step - loss: 0.3694 - acc: 0.8703 - val_loss: 0.2700 - val_acc: 0.9141\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 3s 167ms/step - loss: 0.3438 - acc: 0.8792 - val_loss: 0.2770 - val_acc: 0.9000\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 3s 178ms/step - loss: 0.3614 - acc: 0.8661 - val_loss: 0.3075 - val_acc: 0.8797\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 2s 158ms/step - loss: 0.3005 - acc: 0.8901 - val_loss: 0.2739 - val_acc: 0.8969\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 3s 167ms/step - loss: 0.3101 - acc: 0.8849 - val_loss: 0.2618 - val_acc: 0.9062\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 3s 173ms/step - loss: 0.2751 - acc: 0.8948 - val_loss: 0.2657 - val_acc: 0.9156\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 2s 156ms/step - loss: 0.2669 - acc: 0.9094 - val_loss: 0.3007 - val_acc: 0.8922\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 3s 168ms/step - loss: 0.2949 - acc: 0.9005 - val_loss: 0.2556 - val_acc: 0.9031\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 2s 150ms/step - loss: 0.2684 - acc: 0.9021 - val_loss: 0.2675 - val_acc: 0.8938\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 3s 170ms/step - loss: 0.2440 - acc: 0.9141 - val_loss: 0.2753 - val_acc: 0.8906\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 2s 167ms/step - loss: 0.2398 - acc: 0.9151 - val_loss: 0.2155 - val_acc: 0.9219\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 2s 165ms/step - loss: 0.2445 - acc: 0.9130 - val_loss: 0.2564 - val_acc: 0.8969\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 3s 170ms/step - loss: 0.2712 - acc: 0.9031 - val_loss: 0.2620 - val_acc: 0.8875\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 2s 149ms/step - loss: 0.2469 - acc: 0.9109 - val_loss: 0.2833 - val_acc: 0.8953\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 2s 162ms/step - loss: 0.2289 - acc: 0.9161 - val_loss: 0.2528 - val_acc: 0.9094\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 3s 170ms/step - loss: 0.2251 - acc: 0.9115 - val_loss: 0.2474 - val_acc: 0.9094\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 2s 161ms/step - loss: 0.2259 - acc: 0.9193 - val_loss: 0.2910 - val_acc: 0.8938\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 3s 167ms/step - loss: 0.2170 - acc: 0.9214 - val_loss: 0.2490 - val_acc: 0.9187\n",
            "CPU times: user 34.3 s, sys: 6.14 s, total: 40.5 s\n",
            "Wall time: 1min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_gru(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NP0Ea8yQNprO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "0e3ead06-af38-4d60-8b65-a6ba3ac16036"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-GRU, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_gru(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_4/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.098780155181885 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 1.2433 - acc: 0.4554INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 4.094862937927246 secs\n",
            "15/15 [==============================] - 17s 1s/step - loss: 1.2324 - acc: 0.4646 - val_loss: 0.9973 - val_acc: 0.6906\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 2s 150ms/step - loss: 0.8215 - acc: 0.6995 - val_loss: 0.5217 - val_acc: 0.8094\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 2s 160ms/step - loss: 0.4575 - acc: 0.8344 - val_loss: 0.3071 - val_acc: 0.8812\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 2s 147ms/step - loss: 0.4013 - acc: 0.8552 - val_loss: 0.3105 - val_acc: 0.8922\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 2s 160ms/step - loss: 0.3494 - acc: 0.8760 - val_loss: 0.2930 - val_acc: 0.9000\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 2s 153ms/step - loss: 0.3238 - acc: 0.8891 - val_loss: 0.2430 - val_acc: 0.9062\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 2s 152ms/step - loss: 0.2929 - acc: 0.8979 - val_loss: 0.2267 - val_acc: 0.9141\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 2s 157ms/step - loss: 0.3142 - acc: 0.8969 - val_loss: 0.2876 - val_acc: 0.8938\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 2s 165ms/step - loss: 0.2965 - acc: 0.8917 - val_loss: 0.2605 - val_acc: 0.9109\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 2s 158ms/step - loss: 0.2614 - acc: 0.9036 - val_loss: 0.2307 - val_acc: 0.9203\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 2s 157ms/step - loss: 0.2942 - acc: 0.8969 - val_loss: 0.2559 - val_acc: 0.8984\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 2s 159ms/step - loss: 0.2571 - acc: 0.9151 - val_loss: 0.2691 - val_acc: 0.9031\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 2s 158ms/step - loss: 0.2578 - acc: 0.9104 - val_loss: 0.2763 - val_acc: 0.9062\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 2s 148ms/step - loss: 0.2668 - acc: 0.9109 - val_loss: 0.3123 - val_acc: 0.8938\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 2s 150ms/step - loss: 0.2369 - acc: 0.9182 - val_loss: 0.2459 - val_acc: 0.9187\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 2s 152ms/step - loss: 0.2310 - acc: 0.9161 - val_loss: 0.2408 - val_acc: 0.9094\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 2s 151ms/step - loss: 0.2438 - acc: 0.9089 - val_loss: 0.2435 - val_acc: 0.9078\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 2s 155ms/step - loss: 0.2386 - acc: 0.9151 - val_loss: 0.2640 - val_acc: 0.9172\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 2s 157ms/step - loss: 0.2421 - acc: 0.9208 - val_loss: 0.2628 - val_acc: 0.9031\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 2s 155ms/step - loss: 0.2434 - acc: 0.9104 - val_loss: 0.2781 - val_acc: 0.9000\n",
            "CPU times: user 32.9 s, sys: 5.91 s, total: 38.8 s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RNN-Bidirectional, Word Embeddings\n",
        "# It doesn't work with TPU, but it works on CPU/GPU\n",
        "def create_bidirectional_rnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyS7YPyWOiD4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-Bidirectional, Word Embeddings\n",
        "# It doesn't work with TPU, but it works on CPU/GPU\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "#classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rcnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "#classifier = create_rcnn()\n",
        "#accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "#                       is_neural_net=True, epochs=10)\n",
        "#print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "bec35936-82a9-4d42-c8dc-9fbed1435687"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RCNN, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 1.1720 - acc: 0.4778 - val_loss: 0.8795 - val_acc: 0.6781\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 15s 737ms/step - loss: 0.6697 - acc: 0.7627 - val_loss: 0.3682 - val_acc: 0.8750\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 767ms/step - loss: 0.3951 - acc: 0.8652 - val_loss: 0.2989 - val_acc: 0.8958\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.3331 - acc: 0.8842 - val_loss: 0.2719 - val_acc: 0.9027\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.3086 - acc: 0.8920 - val_loss: 0.2577 - val_acc: 0.9085\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 726ms/step - loss: 0.2913 - acc: 0.8978 - val_loss: 0.2602 - val_acc: 0.9106\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.2788 - acc: 0.9010 - val_loss: 0.2493 - val_acc: 0.9116\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 14s 718ms/step - loss: 0.2728 - acc: 0.9040 - val_loss: 0.2403 - val_acc: 0.9160\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 752ms/step - loss: 0.2595 - acc: 0.9107 - val_loss: 0.2350 - val_acc: 0.9162\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 15s 738ms/step - loss: 0.2504 - acc: 0.9119 - val_loss: 0.2329 - val_acc: 0.9191\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 742ms/step - loss: 0.2422 - acc: 0.9171 - val_loss: 0.2354 - val_acc: 0.9194\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 743ms/step - loss: 0.2377 - acc: 0.9157 - val_loss: 0.2263 - val_acc: 0.9201\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 15s 728ms/step - loss: 0.2304 - acc: 0.9206 - val_loss: 0.2275 - val_acc: 0.9212\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 15s 741ms/step - loss: 0.2250 - acc: 0.9222 - val_loss: 0.2191 - val_acc: 0.9201\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 14s 721ms/step - loss: 0.2167 - acc: 0.9246 - val_loss: 0.2145 - val_acc: 0.9230\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 745ms/step - loss: 0.2133 - acc: 0.9239 - val_loss: 0.2116 - val_acc: 0.9251\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.2073 - acc: 0.9272 - val_loss: 0.2179 - val_acc: 0.9240\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.2040 - acc: 0.9280 - val_loss: 0.2116 - val_acc: 0.9285\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 0.2000 - acc: 0.9299 - val_loss: 0.2085 - val_acc: 0.9282\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.1947 - acc: 0.9312 - val_loss: 0.2079 - val_acc: 0.9284\n",
            "CPU times: user 7min 58s, sys: 59.2 s, total: 8min 57s\n",
            "Wall time: 5min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}