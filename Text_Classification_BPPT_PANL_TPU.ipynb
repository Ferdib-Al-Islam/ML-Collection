{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - BPPT PANL - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "#!pip install -U ntlk\n",
        "#!pip install torch\n",
        "#import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9856b405-4fd9-4439-ef45-dfae05b48557"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "9ba7ac16-b525-499e-c82f-af3c13d7c441"
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "try:\n",
        "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "except KeyError:\n",
        "  TPU_WORKER = None\n",
        "\n",
        "np.random.seed(seed=10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.11.107.34:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18285324125260461732)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5930766872879097493)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 1297665422713504370)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 190512972810837202)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15949622976445792891)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3996167689778050471)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6142117144740768645)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1371746121324670562)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5376950368947805216)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13526067961326447463)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12385896242779205703)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 10818464450329875876)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3bbd7f97-f356-4d1d-a743-06366ccd0e70"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "894381d3-0cf1-4383-ea06-97f95792e5ed"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 3.0G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  32M Oct 10 16:21 bard.h5\n",
            "drwx------ 2 root root 4.0K Oct  9 15:04 BPPTIndToEngCorpus\n",
            "-rw------- 1 root root  25M Oct  9 16:46 wiki.id.10K.vec\n",
            "-rw------- 1 root root 755M Oct  9 16:35 wiki.id.300K.vec\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n",
            "total 9.2M\n",
            "-rw------- 1 root root 4.6M Oct  9 15:04 bppt_panl.csv\n",
            "-rw------- 1 root root 476K Oct  9 15:04 bppt_panl_test.csv\n",
            "-rw------- 1 root root 4.2M Oct  9 15:04 bppt_panl_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7JIIAgpafrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
        "test_df.columns = ['label', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r4rofDLbCPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "384046c7-f6c4-448f-f850-5d4319d29883"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "#print(train_df['label'][:10].values)\n",
        "#print(train_df['text'][:10].values)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      3  Spurs menguasai permainan sejak saat Keane men...\n",
            "1      0  Saat ini juga sedang berlangsung negosiasi den...\n",
            "2      3  Pemain tengah Fenerbahce, Deivid, beralih dari...\n",
            "3      0  Menurut Kepala Perwakilan BPKP di Provinsi Pap...\n",
            "4      0  Indonesia memiliki potensi yang luar biasa seb...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4255e7e5-0d66-495c-ac32-ce31294b1c51"
      },
      "cell_type": "code",
      "source": [
        "#!pip show tensorflow\n",
        "#!nvidia-smi\n",
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COLAB_TPU_ADDR=10.11.107.34:8470\n",
            "DATALAB_SETTINGS_OVERRIDES='{\"datalabBasePath\":\"/tun/m/tpu-qrllmhg0h7vp/\",\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"notebook\",\"-y\",\"--no-browser\",\"--log-level=DEBUG\",\"--debug\",\"--NotebookApp.allow_origin=\\\"*\\\"\",\"--NotebookApp.log_format=\\\"%(message)s\\\"\",\"--NotebookApp.disable_check_xsrf=True\",\"--NotebookApp.token=\",\"--Session.key=\\\"\\\"\",\"--Session.keyfile=\\\"\\\"\",\"--ContentsManager.untitled_directory=\\\"Untitled Folder\\\"\",\"--ContentsManager.untitled_file=\\\"Untitled File\\\"\",\"--ContentsManager.untitled_notebook=\\\"Untitled Notebook\\\"\",\"--KernelManager.autorestart=True\",\"--ip=\\\"172.28.0.2\\\"\"]}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3ebd1f54-da7b-4bc7-9a10-d29b7fdd983f"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19469    Pada petenis unggulan mendapatkan bye pada per...\n",
            "24559    Pencetak gol terbanyak Serie A David Trezeguet...\n",
            "5322     PT Pemeringkat Efek Indonesia Pefindo memberik...\n",
            "15053    Gas untuk keperluan rumah tangga misalnya, mes...\n",
            "20938    Kapten Bayern Munich Oliver Kahn memperingatka...\n",
            "Name: text, dtype: object\n",
            "[3 3 0 0 3]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "Name: text, dtype: object\n",
            "[1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_df['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfzSkzK9OSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(xtrain_count, xvalid_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(train_df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5850d482-2a8e-4e0a-e1f2-b9dd54b53eaf"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki.id.10K.vec')):\n",
        "  if i%10000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
        "  except ValueError:\n",
        "    print(\"Values: {}: {}\".format(i, values))\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train_df['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JNxd48596LbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df['char_count'] = train_df['text'].apply(len)\n",
        "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "train_df['word_density'] = train_df['char_count'] / (train_df['word_count']+1)\n",
        "train_df['punctuation_count'] = train_df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "train_df['title_word_count'] = train_df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "train_df['upper_case_word_count'] = train_df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrGptFBdhuYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "06d38b67-1969-477d-b76d-337f1259f689"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      3  Spurs menguasai permainan sejak saat Keane men...\n",
            "1      0  Saat ini juga sedang berlangsung negosiasi den...\n",
            "2      3  Pemain tengah Fenerbahce, Deivid, beralih dari...\n",
            "3      0  Menurut Kepala Perwakilan BPKP di Provinsi Pap...\n",
            "4      0  Indonesia memiliki potensi yang luar biasa seb...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q32Bm7xp6eq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\" predictions:\", predictions[:20])\n",
        "    print(\"ground truth:\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ef6c0d30-69c5-4066-d5a5-49d2ff572625"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, Count Vectors:  0.9360279150915964\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, WordLevel TF-IDF:  0.9274498400697877\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 1 1 0 0 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, N-Gram Vectors:  0.7916545507414946\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, CharLevel Vectors:  0.8685664437336436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5dcdb419-7dd6-45a7-e8dc-17d11220017f"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, Count Vectors:  0.9520209363186973\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, WordLevel TF-IDF:  0.9358825239895319\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 1 3 0 3 0 1 0 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, N-Gram Vectors:  0.8269845885431811\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, CharLevel Vectors:  0.9017156150043617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "500adb3a-84bd-4ee2-dddb-4d341a38b962"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "SVM, N-Gram Vectors:  0.4287583599883687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3dfa214e-13af-404a-84ee-bf289eab2ce1"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 0 2 0 3 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, Count Vectors:  0.8871765047979063\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 2 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, WordLevel TF-IDF:  0.883396336144228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8583a3df-3a8f-44df-e00a-518e5c32a17a"
      },
      "cell_type": "code",
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, Count Vectors:  0.8200058156440826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 0 2 0 0]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, WordLevel TF-IDF:  0.8220412910729863\n",
            " predictions: [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "ground truth: [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, CharLevel Vectors:  0.8298924105844723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        #print(\">> on_epoch_end:\")\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0929c81f-4723-4145-b1bb-cefec5288ff7"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7500, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  def to_tpu(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "      print(\"TPU Wrapper start\")\n",
        "      print(func)\n",
        "      if TPU_WORKER is not None:\n",
        "        tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "        print(\"TPU exist\")\n",
        "        return tpu_model(*args, **kwargs)\n",
        "      else:\n",
        "        print(\"TPU not exist\")\n",
        "        return func(*args, **kwargs)\n",
        "    print(\"TO_TPU\")\n",
        "    return wrapper\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEgnkciMxoGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_tpu(func):\n",
        "  if TPU_WORKER is not None:\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    return tpu_model\n",
        "  else:\n",
        "    return func\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOK2hrFuYnIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        },
        "outputId": "adbdf4da-c0be-4e44-bbbd-36342ecb3209"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "print(classifier)\n",
        "classifier = to_tpu(classifier)\n",
        "print(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.training.Model object at 0x7fb28231e0b8>\n",
            "TPU Wrapper start\n",
            "<tensorflow.python.keras.engine.training.Model object at 0x7fb28231e0b8>\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "TPU exist\n",
            "<tensorflow.contrib.tpu.python.tpu.keras_support.KerasTPUModel object at 0x7fb282244d30>\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_2/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.8963499069213867 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 1.0140 - acc: 0.6127INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.6671199798583984 secs\n",
            "20/20 [==============================] - 15s 729ms/step - loss: 0.9969 - acc: 0.6211 - val_loss: 0.5085 - val_acc: 0.8568\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 3s 154ms/step - loss: 0.4799 - acc: 0.8328 - val_loss: 0.3295 - val_acc: 0.8906\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.3664 - acc: 0.8699 - val_loss: 0.2732 - val_acc: 0.8997\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 3s 144ms/step - loss: 0.3200 - acc: 0.8926 - val_loss: 0.2492 - val_acc: 0.9115\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 3s 164ms/step - loss: 0.2942 - acc: 0.8980 - val_loss: 0.2303 - val_acc: 0.9258\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 3s 150ms/step - loss: 0.2594 - acc: 0.9141 - val_loss: 0.2155 - val_acc: 0.9245\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 3s 147ms/step - loss: 0.2469 - acc: 0.9137 - val_loss: 0.2579 - val_acc: 0.9089\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 0.2152 - acc: 0.9242 - val_loss: 0.2236 - val_acc: 0.9219\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 3s 148ms/step - loss: 0.2199 - acc: 0.9242 - val_loss: 0.2263 - val_acc: 0.9167\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.2030 - acc: 0.9301 - val_loss: 0.1963 - val_acc: 0.9310\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.1876 - acc: 0.9340 - val_loss: 0.2271 - val_acc: 0.9141\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 3s 143ms/step - loss: 0.1812 - acc: 0.9375 - val_loss: 0.2092 - val_acc: 0.9167\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 3s 144ms/step - loss: 0.1696 - acc: 0.9398 - val_loss: 0.1963 - val_acc: 0.9232\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.1507 - acc: 0.9512 - val_loss: 0.2253 - val_acc: 0.9258\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.1391 - acc: 0.9535 - val_loss: 0.2126 - val_acc: 0.9284\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.1527 - acc: 0.9449 - val_loss: 0.1704 - val_acc: 0.9401\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 3s 149ms/step - loss: 0.1370 - acc: 0.9563 - val_loss: 0.1538 - val_acc: 0.9375\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.1275 - acc: 0.9527 - val_loss: 0.2071 - val_acc: 0.9323\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 153ms/step - loss: 0.1134 - acc: 0.9617 - val_loss: 0.1870 - val_acc: 0.9414\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 3s 146ms/step - loss: 0.0948 - acc: 0.9688 - val_loss: 0.2072 - val_acc: 0.9271\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "CPU times: user 44.5 s, sys: 8.03 s, total: 52.5 s\n",
            "Wall time: 1min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b20d43a1-cbf6-4bce-be25-ebcbb08a9072"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on validation dataset\n",
        "\n",
        "\n",
        "sequences = token.texts_to_sequences(test_df['text'].values)\n",
        "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "\"\"\"\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\"\"\"\n",
        "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
        "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 0s 108us/step\n",
            "Loss for final step: 0.17674767505865135, accuracy: 0.9450261780104712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "diSp_8Sg9rrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "446700b5-a6cf-4835-d617-435fb776b3ff"
      },
      "cell_type": "code",
      "source": [
        "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
        "#print(labels[:5])\n",
        "print(np.array(labels)[:5])\n",
        "print(test_df['label'].values)\n",
        "print(to_categorical(3, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 0]\n",
            " [0 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]]\n",
            "[2 1 0 ... 2 0 2]\n",
            "[0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6c6b0e67-8984-4ca2-fff3-9365424e2bd7"
      },
      "cell_type": "code",
      "source": [
        "score = classifier.predict(test_x_seq, verbose=1)\n",
        "#print(score)\n",
        "print('Loss for final step: {}'.format(np.argmax(score, axis=1)[:10]))\n",
        "print(valid_x[:10])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 0s 77us/step\n",
            "Loss for final step: [2 1 0 0 0 2 2 0 2 0]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "19559    Agri Resources BV, sebuah perusahaan di Beland...\n",
            "6336     Asalkan kita punya program-program yang bagus ...\n",
            "2881     Sikap ini diperkuat oleh hasil interogasi terh...\n",
            "16877            Gurita juga menggunakan sistem yang sama.\n",
            "26868    Tantangan utamanya mungkin akan datang dari Ra...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ace56d9c-a28f-49c0-c191-0398366126b0"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335236     1705600     3564096         804     8065540    12425368\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7f976e14-3199-46b5-e78c-a4962e50c768"
      },
      "cell_type": "code",
      "source": [
        "print(score)\n",
        "print(np.argmax(score, axis=1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.3721889e-04 1.7559213e-04 9.9948490e-01 2.2181157e-06]\n",
            " [1.0414308e-08 9.9999595e-01 2.9743585e-06 1.0576000e-06]\n",
            " [9.7819394e-01 6.5141567e-03 1.5249715e-02 4.2119769e-05]\n",
            " ...\n",
            " [1.3407861e-02 1.9714341e-02 9.6600837e-01 8.6940167e-04]\n",
            " [9.9999952e-01 3.1176335e-09 4.6144635e-07 4.8664687e-11]\n",
            " [2.9894806e-04 7.0751207e-07 9.9969995e-01 4.3038932e-07]]\n",
            "[2 1 0 ... 2 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
        "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
        "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "76420a26-54a2-4953-d3d2-46de64817bb6"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 12s 608ms/step - loss: 1.0240 - acc: 0.5697 - val_loss: 0.5509 - val_acc: 0.8366\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 9s 439ms/step - loss: 0.4237 - acc: 0.8557 - val_loss: 0.2883 - val_acc: 0.8965\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.3061 - acc: 0.8934 - val_loss: 0.2479 - val_acc: 0.9118\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 9s 435ms/step - loss: 0.2696 - acc: 0.9048 - val_loss: 0.2386 - val_acc: 0.9149\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 9s 440ms/step - loss: 0.2481 - acc: 0.9127 - val_loss: 0.2233 - val_acc: 0.9219\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.2391 - acc: 0.9166 - val_loss: 0.2154 - val_acc: 0.9271\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 9s 446ms/step - loss: 0.2238 - acc: 0.9207 - val_loss: 0.2110 - val_acc: 0.9251\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.2166 - acc: 0.9214 - val_loss: 0.2010 - val_acc: 0.9320\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.2109 - acc: 0.9268 - val_loss: 0.2027 - val_acc: 0.9310\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 9s 442ms/step - loss: 0.2061 - acc: 0.9259 - val_loss: 0.2138 - val_acc: 0.9250\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 9s 440ms/step - loss: 0.2039 - acc: 0.9271 - val_loss: 0.2068 - val_acc: 0.9274\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 9s 438ms/step - loss: 0.1989 - acc: 0.9286 - val_loss: 0.1972 - val_acc: 0.9323\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 9s 437ms/step - loss: 0.1866 - acc: 0.9334 - val_loss: 0.1880 - val_acc: 0.9357\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1755 - acc: 0.9358 - val_loss: 0.1929 - val_acc: 0.9323\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 9s 438ms/step - loss: 0.1746 - acc: 0.9360 - val_loss: 0.1844 - val_acc: 0.9372\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1666 - acc: 0.9403 - val_loss: 0.1861 - val_acc: 0.9336\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 9s 434ms/step - loss: 0.1637 - acc: 0.9408 - val_loss: 0.1871 - val_acc: 0.9377\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 9s 433ms/step - loss: 0.1554 - acc: 0.9452 - val_loss: 0.1800 - val_acc: 0.9395\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1530 - acc: 0.9457 - val_loss: 0.1796 - val_acc: 0.9398\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1440 - acc: 0.9497 - val_loss: 0.1773 - val_acc: 0.9408\n",
            "CPU times: user 4min 5s, sys: 25.3 s, total: 4min 30s\n",
            "Wall time: 3min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1204
        },
        "outputId": "923e65b2-4ff4-44fb-e84c-767c091fcb45"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5dc3ae600262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ndef create_rnn_gru():\\n    # Add an Input Layer\\n    input_layer = layers.Input((70, ))\\n\\n    # Add the word embedding Layer\\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\\n\\n    # Add the GRU Layer\\n    lstm_layer = layers.GRU(100)(embedding_layer)\\n\\n    # Add the output Layers\\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\\n\\n    # Compile the model\\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\\n    model.compile(optimizer=optimizers.Adam(), loss=\\'binary_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    \\n    return model\\n\\nclassifier = create_rnn_gru()\\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \\n                       is_neural_net=True, epochs=10)\\nprint(\"RNN-GRU, Word Embeddings\",  accuracy)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-dfa01db8d1c0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 993\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    326\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have shape (70,) but got array with shape (100,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mM-MA8WKRg22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fbde5554-0f4d-4a9e-c3b3-7a2f0b3924e5"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 38s 5ms/step - loss: 0.6029\n",
            "predictions [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "RNN-Bidirectional, Word Embeddings 0.7936\n",
            "CPU times: user 1min 21s, sys: 1.16 s, total: 1min 22s\n",
            "Wall time: 45.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "cf50faff-2384-4456-c5b0-c70ef7f89f25"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7500/7500 [==============================] - 12s 2ms/step - loss: 0.5678\n",
            "Epoch 2/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3829\n",
            "Epoch 3/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3374\n",
            "Epoch 4/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2814\n",
            "Epoch 5/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2365\n",
            "Epoch 6/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2043\n",
            "Epoch 7/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1675\n",
            "Epoch 8/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1320\n",
            "Epoch 9/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1081\n",
            "Epoch 10/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1005\n",
            "predictions [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "CNN, Word Embeddings 0.8624\n",
            "CPU times: user 3min 22s, sys: 4.89 s, total: 3min 27s\n",
            "Wall time: 1min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}