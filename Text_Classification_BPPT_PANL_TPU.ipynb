{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - BPPT PANL - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "FbiTZZmczpi5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Indonesian Text Classification\n",
        "\n",
        "This is a text classification  of indonesian corpus using several different technique such as Naive Bayes, SVM, Random Forest, Convolition Neural Network (CNN), LSTM or GRU. An indonesian [pre-trained word vectors](https://fasttext.cc/docs/en/pretrained-vectors.html) from FastText has ben also used in our Neural Network models.\n",
        "We use [Word Bahasa Indonesia Corpus and Parallel English Translation](https://www.panl10n.net/english/outputs/Indonesia/BPPT/0902/BPPTIndToEngCorpusHalfM.zip) dataset from PAN Localization.\n",
        "It contains 500,000 words from various online sources translated into English.\n",
        "For our text classification, we use only the indonesian part.\n",
        "The corpus has 4 classes:\n",
        "  - 0: Economy\n",
        "  - 1: International\n",
        "  - 2: Science\n",
        "  - 3: Sport\n",
        " \n",
        "Originally each class is in separate file, we combine, randomize and split it to train and test file with 90:10.  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "try:\n",
        "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "      tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "except KeyError:\n",
        "  TPU_WORKER = None\n",
        "\n",
        "np.random.seed(seed=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e063f0a8-feb9-4d86-9b69-2d7dd6ae5024"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c0083857-9151-4292-de0e-02c0e0153dcd"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 3.0G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  31M Oct 11 19:03 bard.h5\n",
            "drwx------ 2 root root 4.0K Oct  9 15:04 BPPTIndToEngCorpus\n",
            "-rw------- 1 root root  25M Oct  9 16:46 wiki.id.10K.vec\n",
            "-rw------- 1 root root 755M Oct  9 16:35 wiki.id.300K.vec\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n",
            "total 7.1M\n",
            "-rw------- 1 root root 3.6M Oct 11 10:45 bppt_panl.csv\n",
            "-rw------- 1 root root 362K Oct 11 10:45 bppt_panl_test.csv\n",
            "-rw------- 1 root root 3.2M Oct 11 10:45 bppt_panl_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7JIIAgpafrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
        "test_df.columns = ['label', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r4rofDLbCPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d9c0498c-a4f6-4b63-94d1-a26e5e2524dc"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "#print(train_df['label'][:10].values)\n",
        "#print(train_df['text'][:10].values)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      0  Pertumbuhan ekonomi 2007 yang diproyeksikan me...\n",
            "1      3  Pelatih Real Bernd Schuster harus mengeluarkan...\n",
            "2      2  Laporan itu adalah pengumuman kedua dari badan...\n",
            "3      0  Lonjakan laba bersih tersebut, selain didorong...\n",
            "4      3  LeBron James menyumbang 24 poin, 11 assist dan...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "595bcbda-674c-4eff-d693-67c0be4abfb5"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19555    Layanan fantastis ini membutuhkan waktu lama u...\n",
            "9774     Gasparotto memimpin lomba keseluruhan disusul ...\n",
            "10459    Pereli Prancis itu unggul dua menit 33,2 detik...\n",
            "19916    Karena itu Antam juga melakukan diversifikasi ...\n",
            "15343    Para kapitalis ini menginginkan pemerintah Ind...\n",
            "Name: text, dtype: object\n",
            "[2 3 3 0 0]\n",
            "3081     Menurut Shahab, peralatan Rig pada BJP-1R1 sud...\n",
            "1601     Anak pepsis memakan daging tarantula dan berli...\n",
            "20221    Dalam hal kepemilikan asset, maka sektor-sekto...\n",
            "2118     Ilmuwan telah menemukan informasi genetik yang...\n",
            "17469    Anda menyesuaikan dengan itu dan lawan juga sama.\n",
            "Name: text, dtype: object\n",
            "[0 2 0 2 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_df['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(train_df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e917deea-159f-40d9-c33a-3d832816b779"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki.id.300K.vec')):\n",
        "  if i%50000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
        "  except ValueError:\n",
        "    print(\"Values: {}: {}\".format(i, values))\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train_df['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "50000\n",
            "100000\n",
            "150000\n",
            "200000\n",
            "250000\n",
            "300000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DkNjxT1SEKZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Konventional Machine Learning"
      ]
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\" predictions:\", predictions[:20])\n",
        "    print(\"ground truth:\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEkAoXG_GAVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "52cbf831-5869-4c3f-c711-5823dda594b5"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 3 0 0 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, Count Vectors:  0.9269195189639223\n",
            " predictions: [0 2 0 2 3 0 1 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, WordLevel TF-IDF:  0.9161887141535615\n",
            " predictions: [0 2 0 2 1 0 1 2 0 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, N-Gram Vectors:  0.7822386679000926\n",
            " predictions: [0 2 0 2 2 0 0 2 2 1 2 2 0 0 1 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "NB, CharLevel Vectors:  0.8432932469935245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QA6ro7LjGLFj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear Classifier"
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "401fe193-6e5f-40c7-9ed7-92c12e0d59ab"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 2 0 0 2 2 1 0 2 2 0 0 1 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, Count Vectors:  0.9265494912118409\n",
            " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, WordLevel TF-IDF:  0.9178538390379278\n",
            " predictions: [0 2 2 2 1 0 1 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, N-Gram Vectors:  0.8085106382978723\n",
            " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 2 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "LR, CharLevel Vectors:  0.8888066604995375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ebqjOIqGYfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4c507e05-b62a-4f99-8aa9-24149938ccc1"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "# It seems there is something wrong here, since the predictions are always 0\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "SVM, N-Gram Vectors:  0.25827937095282144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzKTmXIKGdi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e9b94aeb-47b5-4e26-cb28-b282ddbe29c5"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 0 2 2 0 0 2 2 1 0 2 2 0 1 0 3 1 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "RF, Count Vectors:  0.8392229417206291\n",
            " predictions: [0 2 0 2 1 0 0 2 0 1 0 2 3 0 0 0 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "RF, WordLevel TF-IDF:  0.8338575393154487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8h3s81UGkIB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Extreme Gradient Boosting"
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "26749603-6643-42b8-f43d-f9030a336f5c"
      },
      "cell_type": "code",
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, Count Vectors:  0.808695652173913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, WordLevel TF-IDF:  0.8070305272895467\n",
            " predictions: [0 2 0 2 2 0 1 2 2 2 0 2 2 0 1 2 3 2 1 3]\n",
            "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
            "Xgb, CharLevel Vectors:  0.8201665124884366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1KenKn_bDw4k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOTyMac-ScU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-ZDXV--SySi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tpu_wrapper(func):\n",
        "  if TPU_WORKER is not None:\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    return tpu_model\n",
        "  else:\n",
        "    return func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This model doesn't work with current dataset\n",
        "def create_simple_model(input_length=100):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_length, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(4, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "#classifier = create_simple_model(xtrain_tfidf_ngram.shape[1])\n",
        "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "#                       is_neural_net=True, epochs=1)\n",
        "#print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI6s_qBsSjzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "#NN, Ngram Level TF IDF Vectors\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_simple_model(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e6788c1-ee70-4e55-9081-43c99d39a967"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16214, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def to_tpu(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "      print(\"TPU Wrapper start\")\n",
        "      print(func)\n",
        "      if TPU_WORKER is not None:\n",
        "        tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "        print(\"TPU exist\")\n",
        "        return tpu_model(*args, **kwargs)\n",
        "      else:\n",
        "        print(\"TPU not exist\")\n",
        "        return func(*args, **kwargs)\n",
        "    print(\"TO_TPU\")\n",
        "    return wrapper\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JuHG89VmFdp5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ]
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "bd7f5767-c741-45bd-bf1c-988435c77c45"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 4s 289ms/step - loss: 1.1967 - acc: 0.4941 - val_loss: 0.7733 - val_acc: 0.8369\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 2s 114ms/step - loss: 0.6243 - acc: 0.8046 - val_loss: 0.4103 - val_acc: 0.8627\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 2s 138ms/step - loss: 0.4095 - acc: 0.8594 - val_loss: 0.3286 - val_acc: 0.8775\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 2s 126ms/step - loss: 0.3417 - acc: 0.8813 - val_loss: 0.2932 - val_acc: 0.8906\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 2s 141ms/step - loss: 0.3069 - acc: 0.8923 - val_loss: 0.2733 - val_acc: 0.8982\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 2s 122ms/step - loss: 0.2855 - acc: 0.8998 - val_loss: 0.2594 - val_acc: 0.9047\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 2s 126ms/step - loss: 0.2601 - acc: 0.9102 - val_loss: 0.2487 - val_acc: 0.9064\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 2s 129ms/step - loss: 0.2470 - acc: 0.9155 - val_loss: 0.2367 - val_acc: 0.9123\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 2s 124ms/step - loss: 0.2275 - acc: 0.9207 - val_loss: 0.2294 - val_acc: 0.9174\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 2s 119ms/step - loss: 0.2206 - acc: 0.9206 - val_loss: 0.2243 - val_acc: 0.9187\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 2s 134ms/step - loss: 0.2108 - acc: 0.9275 - val_loss: 0.2184 - val_acc: 0.9221\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 2s 123ms/step - loss: 0.1965 - acc: 0.9331 - val_loss: 0.2154 - val_acc: 0.9221\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 2s 120ms/step - loss: 0.1826 - acc: 0.9376 - val_loss: 0.2109 - val_acc: 0.9236\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 2s 128ms/step - loss: 0.1728 - acc: 0.9385 - val_loss: 0.2102 - val_acc: 0.9248\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 2s 129ms/step - loss: 0.1626 - acc: 0.9439 - val_loss: 0.2042 - val_acc: 0.9250\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 2s 124ms/step - loss: 0.1587 - acc: 0.9461 - val_loss: 0.2035 - val_acc: 0.9252\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 2s 128ms/step - loss: 0.1472 - acc: 0.9507 - val_loss: 0.2005 - val_acc: 0.9238\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 2s 115ms/step - loss: 0.1368 - acc: 0.9544 - val_loss: 0.1989 - val_acc: 0.9270\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 2s 139ms/step - loss: 0.1325 - acc: 0.9551 - val_loss: 0.1985 - val_acc: 0.9289\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 2s 122ms/step - loss: 0.1239 - acc: 0.9570 - val_loss: 0.1986 - val_acc: 0.9279\n",
            "CPU times: user 43.1 s, sys: 9.25 s, total: 52.4 s\n",
            "Wall time: 45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0afb27ed-e6b8-4efb-dc50-5ec6271a972a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'cnn.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68eb633f-2ad1-4519-8086-b85d9c44abd7"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on test dataset\n",
        "\n",
        "test_x = test_df['text'].values\n",
        "sequences = token.texts_to_sequences(test_x)\n",
        "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "# TPU can be enabled here if we need  \n",
        "# classifier = tpu_wrapper(classifier)\n",
        "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
        "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2402/2402 [==============================] - 0s 205us/step\n",
            "Loss for final step: 0.20907926316265263, accuracy: 0.925895087427144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "diSp_8Sg9rrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Just for testing\n",
        "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
        "#print(labels[:5])\n",
        "print(np.array(labels)[:5])\n",
        "print(test_df['label'].values)\n",
        "print(to_categorical(3, 4))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "ac7e333b-e9b6-4a75-8982-e7db2c43f6ca"
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(test_x_seq, verbose=1)\n",
        "#print(prediction)\n",
        "print('Predictions for final step: {}'.format(np.argmax(prediction, axis=1)[:10]))\n",
        "print(test_x[:10])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2402/2402 [==============================] - 0s 81us/step\n",
            "Predictions for final step: [0 0 2 3 1 0 0 1 2 2]\n",
            "['Paradoksnya di sisi lain, sinyal akan diakuinya keberadaan lembaga keuangan mikro non formal juga membuat kegelisahan bagi pelakunya.'\n",
            " 'Menurut dia, harga minyak mentah dunia saat ini berada di posisi 126 dolar AS per barel sedikit melemah dibanding hari sebelumnya yang mencapai 127 dolar AS lebih.'\n",
            " 'Pengkajian ini memberikan konfirmasi bahwa kita menghadapi sejenis materi yang berbeda sama sekali, tak seperti yang kita bayangkan.'\n",
            " 'Gol akhir Marco Borriello membuat Genoa mendapat satu angka setelah striker asal Honduras, David Suazo, mencetak angka pada menit ke-12 untuk Inter, yang pemain tengahnya dari Portugal, Pele, dikeluarkan dari lapangan karena mendapat kartu kuning kedua sebelum turun minum.'\n",
            " 'Namun, bandar udara itu ditutup sebagai langkah pencegahan.'\n",
            " 'Departemen Keuangan telah menetapkan 16 calon agen penjual obligasi negara ritel ORI pada 2007 melalui SK Dirjen Pengelolaan Utang Depkeu No KEP-07/PU/2007 tertanggal 19 Februari 2007.'\n",
            " 'Kontrak kerja sama yang tekah ditandatangani dengan China mencapai puluhan juta dollar AS.'\n",
            " 'Sebab kejadian tersebut masih dalam penyelidikan, kata Letnan Kolonel Josslyn Aberle di Bagdad.'\n",
            " 'Kisah Di Balik Suatu Komunikasi Singkat.'\n",
            " 'Proses degradasi material organik ini tanpa melibatkan oksigen disebut anaerobik digestion Gas yang dihasilkan sebagian besar berupa metana.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d90d8629-1eff-4fcf-cc74-4cabe26d12ae"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335204     1260560     8939392         804     3135252    12365508\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5f212213-4fc3-4501-baa2-4be082e584f8"
      },
      "cell_type": "code",
      "source": [
        "print(prediction)\n",
        "print(np.argmax(prediction, axis=1))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.9899882e-01 4.6373406e-04 5.3280470e-04 4.7004282e-06]\n",
            " [9.9977285e-01 1.3875878e-04 7.5860429e-05 1.2497356e-05]\n",
            " [1.0611779e-01 4.8861925e-02 8.3545119e-01 9.5691448e-03]\n",
            " ...\n",
            " [1.1416495e-02 9.8778975e-01 4.8050229e-04 3.1316865e-04]\n",
            " [1.4843496e-03 5.0487202e-02 4.8084562e-03 9.4321996e-01]\n",
            " [1.4797889e-02 2.0483567e-03 9.8280776e-01 3.4594335e-04]]\n",
            "[0 0 2 ... 1 3 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r0gd-JC7Fj2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Kim Yoon’s CNN"
      ]
    },
    {
      "metadata": {
        "id": "tLhmuqLZ1jIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following model is similar to \n",
        "# Kim Yoon’s Convolutional Neural Networks for Sentence Classification\n",
        "# (https://arxiv.org/abs/1408.5882)\n",
        "#\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 300\n",
        "filter_sizes = (3, 5, 7)\n",
        "num_filters = 100\n",
        "dropout_prob = (0.5, 0.5)\n",
        "hidden_dims = 50\n",
        "\n",
        "def create_cnn_kimyoon(input_length=100):  \n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_dim, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(dropout_prob[0])(embedding_layer)\n",
        "    \n",
        "    conv_array = []\n",
        "    for sz in filter_sizes:\n",
        "        conv = layers.Convolution1D(filters=num_filters,\n",
        "                             kernel_size=sz,\n",
        "                             padding=\"valid\",\n",
        "                             activation=\"relu\",\n",
        "                             strides=1)(embedding_layer)\n",
        "        conv = layers.MaxPooling1D(pool_size=2)(conv)\n",
        "        conv = layers.Flatten()(conv)\n",
        "        conv_array.append(conv)\n",
        "    \n",
        "    layer = layers.Concatenate()(conv_array) if len(conv_array) > 1 else conv_array[0]\n",
        "    \n",
        "    layer = layers.Dropout(dropout_prob[1])(layer)\n",
        "    layer = layers.Dense(hidden_dims, activation=\"relu\")(layer)\n",
        "    output_layer = layers.Dense(4, activation=\"softmax\")(layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SD7eUpGQ7s0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "fdee2ab9-988f-4517-9b73-74103b8cadfb"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn_kimyoon(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'cnn_kimyoon.h5'), overwrite=True)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 6s 385ms/step - loss: 1.0868 - acc: 0.5242 - val_loss: 0.5332 - val_acc: 0.8217\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 4s 239ms/step - loss: 0.4959 - acc: 0.8238 - val_loss: 0.3524 - val_acc: 0.8695\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 4s 238ms/step - loss: 0.4111 - acc: 0.8513 - val_loss: 0.3162 - val_acc: 0.8943\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 3s 226ms/step - loss: 0.3709 - acc: 0.8645 - val_loss: 0.2937 - val_acc: 0.9002\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 4s 239ms/step - loss: 0.3413 - acc: 0.8755 - val_loss: 0.2892 - val_acc: 0.9021\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 3s 226ms/step - loss: 0.3251 - acc: 0.8835 - val_loss: 0.2829 - val_acc: 0.9043\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 3s 231ms/step - loss: 0.3097 - acc: 0.8870 - val_loss: 0.2748 - val_acc: 0.9072\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 3s 227ms/step - loss: 0.2958 - acc: 0.8920 - val_loss: 0.2625 - val_acc: 0.9090\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 3s 226ms/step - loss: 0.2768 - acc: 0.8995 - val_loss: 0.2593 - val_acc: 0.9086\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 4s 238ms/step - loss: 0.2557 - acc: 0.9105 - val_loss: 0.2536 - val_acc: 0.9117\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 4s 236ms/step - loss: 0.2431 - acc: 0.9133 - val_loss: 0.2483 - val_acc: 0.9148\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 3s 224ms/step - loss: 0.2328 - acc: 0.9175 - val_loss: 0.2544 - val_acc: 0.9121\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 4s 239ms/step - loss: 0.2350 - acc: 0.9150 - val_loss: 0.2438 - val_acc: 0.9152\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 4s 239ms/step - loss: 0.2150 - acc: 0.9238 - val_loss: 0.2406 - val_acc: 0.9166\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 4s 237ms/step - loss: 0.2078 - acc: 0.9251 - val_loss: 0.2439 - val_acc: 0.9123\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 4s 237ms/step - loss: 0.2016 - acc: 0.9257 - val_loss: 0.2427 - val_acc: 0.9137\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 4s 242ms/step - loss: 0.1912 - acc: 0.9299 - val_loss: 0.2313 - val_acc: 0.9186\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 4s 234ms/step - loss: 0.1802 - acc: 0.9360 - val_loss: 0.2258 - val_acc: 0.9168\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 3s 229ms/step - loss: 0.1734 - acc: 0.9365 - val_loss: 0.2244 - val_acc: 0.9182\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 4s 234ms/step - loss: 0.1639 - acc: 0.9421 - val_loss: 0.2258 - val_acc: 0.9166\n",
            "CPU times: user 58.6 s, sys: 17.6 s, total: 1min 16s\n",
            "Wall time: 1min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN-LSTM"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
        "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
        "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "302ab4c1-19a3-40ec-f998-275935cbba5e"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# Speed comparison between CPU vs TPU\n",
        "# First we test CPU with 2 epochs\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=2\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "15/15 [==============================] - 9s 627ms/step - loss: 1.1598 - acc: 0.5539 - val_loss: 0.6460 - val_acc: 0.8254\n",
            "Epoch 2/2\n",
            "15/15 [==============================] - 6s 422ms/step - loss: 0.4805 - acc: 0.8472 - val_loss: 0.3420 - val_acc: 0.8754\n",
            "CPU times: user 20.4 s, sys: 2.24 s, total: 22.6 s\n",
            "Wall time: 16.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MuKd3kFMpzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "a13af0ba-3544-48e7-af40-617e82e0997b"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-LSTM, Word Embeddings\n",
        "# We test now the TPU\n",
        "# The result is:\n",
        "# CPU: 65s/epoch\n",
        "# TPU: 3.15s/epoch\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 9s 632ms/step - loss: 1.1334 - acc: 0.5769 - val_loss: 0.6212 - val_acc: 0.8119\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 6s 426ms/step - loss: 0.4754 - acc: 0.8382 - val_loss: 0.3575 - val_acc: 0.8678\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 6s 422ms/step - loss: 0.3443 - acc: 0.8812 - val_loss: 0.2856 - val_acc: 0.8977\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 6s 426ms/step - loss: 0.2977 - acc: 0.8954 - val_loss: 0.2644 - val_acc: 0.9029\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 6s 426ms/step - loss: 0.2859 - acc: 0.8988 - val_loss: 0.2579 - val_acc: 0.9059\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 6s 417ms/step - loss: 0.2666 - acc: 0.9070 - val_loss: 0.2463 - val_acc: 0.9119\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 6s 415ms/step - loss: 0.2505 - acc: 0.9117 - val_loss: 0.2372 - val_acc: 0.9119\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 6s 417ms/step - loss: 0.2406 - acc: 0.9161 - val_loss: 0.2312 - val_acc: 0.9176\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 6s 420ms/step - loss: 0.2357 - acc: 0.9174 - val_loss: 0.2395 - val_acc: 0.9176\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 6s 423ms/step - loss: 0.2353 - acc: 0.9168 - val_loss: 0.2355 - val_acc: 0.9191\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 6s 424ms/step - loss: 0.2337 - acc: 0.9169 - val_loss: 0.2448 - val_acc: 0.9111\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 6s 429ms/step - loss: 0.2220 - acc: 0.9225 - val_loss: 0.2218 - val_acc: 0.9232\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 6s 426ms/step - loss: 0.2218 - acc: 0.9205 - val_loss: 0.2293 - val_acc: 0.9174\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 6s 419ms/step - loss: 0.2143 - acc: 0.9264 - val_loss: 0.2302 - val_acc: 0.9186\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 6s 422ms/step - loss: 0.2026 - acc: 0.9303 - val_loss: 0.2268 - val_acc: 0.9191\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 6s 419ms/step - loss: 0.1996 - acc: 0.9286 - val_loss: 0.2165 - val_acc: 0.9244\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 6s 421ms/step - loss: 0.1934 - acc: 0.9317 - val_loss: 0.2172 - val_acc: 0.9230\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 6s 426ms/step - loss: 0.1866 - acc: 0.9352 - val_loss: 0.2215 - val_acc: 0.9215\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 6s 425ms/step - loss: 0.1865 - acc: 0.9350 - val_loss: 0.2113 - val_acc: 0.9238\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 6s 427ms/step - loss: 0.1778 - acc: 0.9354 - val_loss: 0.2064 - val_acc: 0.9268\n",
            "CPU times: user 2min 52s, sys: 18.9 s, total: 3min 11s\n",
            "Wall time: 2min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RDuZt62jE4WH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN-GRU"
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_gru(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NP0Ea8yQNprO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "8c4c225e-9490-4d65-8daa-56733e6959f4"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-GRU, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_gru(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 8s 559ms/step - loss: 1.2750 - acc: 0.4177 - val_loss: 1.0444 - val_acc: 0.6666\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 5s 363ms/step - loss: 0.8752 - acc: 0.6757 - val_loss: 0.5467 - val_acc: 0.8154\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 0.4949 - acc: 0.8301 - val_loss: 0.3342 - val_acc: 0.8836\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 5s 357ms/step - loss: 0.3629 - acc: 0.8741 - val_loss: 0.2751 - val_acc: 0.8992\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 5s 355ms/step - loss: 0.3151 - acc: 0.8926 - val_loss: 0.2546 - val_acc: 0.9062\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 0.2959 - acc: 0.8983 - val_loss: 0.2453 - val_acc: 0.9086\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 5s 363ms/step - loss: 0.2830 - acc: 0.9013 - val_loss: 0.2436 - val_acc: 0.9115\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 5s 365ms/step - loss: 0.2717 - acc: 0.9053 - val_loss: 0.2338 - val_acc: 0.9121\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.2562 - acc: 0.9122 - val_loss: 0.2267 - val_acc: 0.9180\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.2480 - acc: 0.9141 - val_loss: 0.2243 - val_acc: 0.9180\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.2443 - acc: 0.9170 - val_loss: 0.2186 - val_acc: 0.9176\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 5s 358ms/step - loss: 0.2243 - acc: 0.9249 - val_loss: 0.2115 - val_acc: 0.9240\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 5s 362ms/step - loss: 0.2235 - acc: 0.9220 - val_loss: 0.2128 - val_acc: 0.9223\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 0.2152 - acc: 0.9266 - val_loss: 0.2095 - val_acc: 0.9238\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 5s 365ms/step - loss: 0.2136 - acc: 0.9261 - val_loss: 0.2112 - val_acc: 0.9238\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 0.2066 - acc: 0.9277 - val_loss: 0.2059 - val_acc: 0.9254\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 5s 356ms/step - loss: 0.2018 - acc: 0.9309 - val_loss: 0.1996 - val_acc: 0.9264\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 5s 360ms/step - loss: 0.1911 - acc: 0.9350 - val_loss: 0.1985 - val_acc: 0.9293\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 5s 358ms/step - loss: 0.1881 - acc: 0.9363 - val_loss: 0.1990 - val_acc: 0.9279\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 5s 357ms/step - loss: 0.1868 - acc: 0.9364 - val_loss: 0.2066 - val_acc: 0.9254\n",
            "CPU times: user 2min 32s, sys: 17.4 s, total: 2min 50s\n",
            "Wall time: 1min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JGxJGMV4E-kV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Biderectional RNN"
      ]
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RNN-Bidirectional, Word Embeddings\n",
        "# It doesn't work with TPU, but it works on CPU/GPU\n",
        "def create_bidirectional_rnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyS7YPyWOiD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "b78a6c6d-b15e-4080-ca5e-36b9fd854f16"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RNN-Bidirectional, Word Embeddings\n",
        "# It doesn't work with TPU, but it works on CPU/GPU\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "#classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 14s 904ms/step - loss: 1.2845 - acc: 0.4305 - val_loss: 1.0628 - val_acc: 0.6744\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 9s 629ms/step - loss: 0.8630 - acc: 0.6880 - val_loss: 0.5020 - val_acc: 0.8223\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 9s 623ms/step - loss: 0.4589 - acc: 0.8399 - val_loss: 0.3252 - val_acc: 0.8834\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 9s 624ms/step - loss: 0.3468 - acc: 0.8801 - val_loss: 0.2694 - val_acc: 0.9000\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 10s 635ms/step - loss: 0.3066 - acc: 0.8952 - val_loss: 0.2637 - val_acc: 0.9033\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 9s 624ms/step - loss: 0.2875 - acc: 0.9001 - val_loss: 0.2420 - val_acc: 0.9133\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 9s 630ms/step - loss: 0.2710 - acc: 0.9057 - val_loss: 0.2350 - val_acc: 0.9102\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 9s 619ms/step - loss: 0.2669 - acc: 0.9061 - val_loss: 0.2409 - val_acc: 0.9137\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 9s 629ms/step - loss: 0.2514 - acc: 0.9135 - val_loss: 0.2248 - val_acc: 0.9166\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 9s 618ms/step - loss: 0.2462 - acc: 0.9145 - val_loss: 0.2164 - val_acc: 0.9217\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 10s 640ms/step - loss: 0.2357 - acc: 0.9197 - val_loss: 0.2228 - val_acc: 0.9172\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 9s 632ms/step - loss: 0.2293 - acc: 0.9194 - val_loss: 0.2183 - val_acc: 0.9205\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 10s 645ms/step - loss: 0.2193 - acc: 0.9225 - val_loss: 0.2097 - val_acc: 0.9234\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 9s 631ms/step - loss: 0.2180 - acc: 0.9237 - val_loss: 0.2098 - val_acc: 0.9211\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 9s 628ms/step - loss: 0.2061 - acc: 0.9298 - val_loss: 0.2070 - val_acc: 0.9248\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 9s 615ms/step - loss: 0.2056 - acc: 0.9297 - val_loss: 0.2065 - val_acc: 0.9252\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 9s 621ms/step - loss: 0.1993 - acc: 0.9305 - val_loss: 0.2170 - val_acc: 0.9215\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 9s 626ms/step - loss: 0.1972 - acc: 0.9317 - val_loss: 0.2010 - val_acc: 0.9256\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 9s 631ms/step - loss: 0.1855 - acc: 0.9367 - val_loss: 0.2007 - val_acc: 0.9260\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 9s 622ms/step - loss: 0.1818 - acc: 0.9365 - val_loss: 0.1951 - val_acc: 0.9277\n",
            "CPU times: user 4min 55s, sys: 40.7 s, total: 5min 36s\n",
            "Wall time: 3min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PCNhpBuYFOLI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RCNN"
      ]
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rcnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "bec35936-82a9-4d42-c8dc-9fbed1435687"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# RCNN, Word Embeddings\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
        "classifier = tpu_wrapper(classifier)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 20s 1s/step - loss: 1.1720 - acc: 0.4778 - val_loss: 0.8795 - val_acc: 0.6781\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 15s 737ms/step - loss: 0.6697 - acc: 0.7627 - val_loss: 0.3682 - val_acc: 0.8750\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 767ms/step - loss: 0.3951 - acc: 0.8652 - val_loss: 0.2989 - val_acc: 0.8958\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.3331 - acc: 0.8842 - val_loss: 0.2719 - val_acc: 0.9027\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.3086 - acc: 0.8920 - val_loss: 0.2577 - val_acc: 0.9085\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 15s 726ms/step - loss: 0.2913 - acc: 0.8978 - val_loss: 0.2602 - val_acc: 0.9106\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.2788 - acc: 0.9010 - val_loss: 0.2493 - val_acc: 0.9116\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 14s 718ms/step - loss: 0.2728 - acc: 0.9040 - val_loss: 0.2403 - val_acc: 0.9160\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 15s 752ms/step - loss: 0.2595 - acc: 0.9107 - val_loss: 0.2350 - val_acc: 0.9162\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 15s 738ms/step - loss: 0.2504 - acc: 0.9119 - val_loss: 0.2329 - val_acc: 0.9191\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 15s 742ms/step - loss: 0.2422 - acc: 0.9171 - val_loss: 0.2354 - val_acc: 0.9194\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 15s 743ms/step - loss: 0.2377 - acc: 0.9157 - val_loss: 0.2263 - val_acc: 0.9201\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 15s 728ms/step - loss: 0.2304 - acc: 0.9206 - val_loss: 0.2275 - val_acc: 0.9212\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 15s 741ms/step - loss: 0.2250 - acc: 0.9222 - val_loss: 0.2191 - val_acc: 0.9201\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 14s 721ms/step - loss: 0.2167 - acc: 0.9246 - val_loss: 0.2145 - val_acc: 0.9230\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 15s 745ms/step - loss: 0.2133 - acc: 0.9239 - val_loss: 0.2116 - val_acc: 0.9251\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 712ms/step - loss: 0.2073 - acc: 0.9272 - val_loss: 0.2179 - val_acc: 0.9240\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 15s 761ms/step - loss: 0.2040 - acc: 0.9280 - val_loss: 0.2116 - val_acc: 0.9285\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 0.2000 - acc: 0.9299 - val_loss: 0.2085 - val_acc: 0.9282\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 15s 754ms/step - loss: 0.1947 - acc: 0.9312 - val_loss: 0.2079 - val_acc: 0.9284\n",
            "CPU times: user 7min 58s, sys: 59.2 s, total: 8min 57s\n",
            "Wall time: 5min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}