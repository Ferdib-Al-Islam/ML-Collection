{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - BPPT PANL - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "#!pip install -U ntlk\n",
        "#!pip install torch\n",
        "#import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79143830-97f0-4b10-c29c-6dcecf91c54e"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "#TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "np.random.seed(seed=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "02e94135-5b5b-4de4-a0d6-25c2d1041d8f"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "3344a938-d366-4a05-eaab-113943790486"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 3.0G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  32M Oct 10 09:32 bard.h5\n",
            "drwx------ 2 root root 4.0K Oct  9 15:04 BPPTIndToEngCorpus\n",
            "-rw------- 1 root root  25M Oct  9 16:46 wiki.id.10K.vec\n",
            "-rw------- 1 root root 755M Oct  9 16:35 wiki.id.300K.vec\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n",
            "total 9.2M\n",
            "-rw------- 1 root root 4.6M Oct  9 15:04 bppt_panl.csv\n",
            "-rw------- 1 root root 476K Oct  9 15:04 bppt_panl_test.csv\n",
            "-rw------- 1 root root 4.2M Oct  9 15:04 bppt_panl_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7JIIAgpafrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
        "test_df.columns = ['label', 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5r4rofDLbCPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3f49368c-5d1d-4d4b-fdd9-6f1f9ce499a1"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "#print(train_df['label'][:10].values)\n",
        "#print(train_df['text'][:10].values)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      3  Spurs menguasai permainan sejak saat Keane men...\n",
            "1      0  Saat ini juga sedang berlangsung negosiasi den...\n",
            "2      3  Pemain tengah Fenerbahce, Deivid, beralih dari...\n",
            "3      0  Menurut Kepala Perwakilan BPKP di Provinsi Pap...\n",
            "4      0  Indonesia memiliki potensi yang luar biasa seb...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip show tensorflow\n",
        "#!nvidia-smi\n",
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3c1bbfc0-bb54-4ed2-e724-aef6b546fb0f"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19469    Pada petenis unggulan mendapatkan bye pada per...\n",
            "24559    Pencetak gol terbanyak Serie A David Trezeguet...\n",
            "5322     PT Pemeringkat Efek Indonesia Pefindo memberik...\n",
            "15053    Gas untuk keperluan rumah tangga misalnya, mes...\n",
            "20938    Kapten Bayern Munich Oliver Kahn memperingatka...\n",
            "Name: text, dtype: object\n",
            "[3 3 0 0 3]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "Name: text, dtype: object\n",
            "[1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(train_df['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfzSkzK9OSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(xtrain_count, xvalid_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(train_df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "234c1247-31da-470c-d1f6-f92591b6ca9f"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki.id.300K.vec')):\n",
        "  if i%10000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  try:\n",
        "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
        "  except ValueError:\n",
        "    print(\"Values: {}: {}\".format(i, values))\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train_df['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
        "                                     maxlen=params['max_len'])\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n",
            "210000\n",
            "220000\n",
            "230000\n",
            "240000\n",
            "250000\n",
            "260000\n",
            "270000\n",
            "280000\n",
            "290000\n",
            "300000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JNxd48596LbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df['char_count'] = train_df['text'].apply(len)\n",
        "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "train_df['word_density'] = train_df['char_count'] / (train_df['word_count']+1)\n",
        "train_df['punctuation_count'] = train_df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "train_df['title_word_count'] = train_df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "train_df['upper_case_word_count'] = train_df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrGptFBdhuYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "daaab2c3-d7c1-4030-ac83-56cb96f83036"
      },
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      3  Spurs menguasai permainan sejak saat Keane men...\n",
            "1      0  Saat ini juga sedang berlangsung negosiasi den...\n",
            "2      3  Pemain tengah Fenerbahce, Deivid, beralih dari...\n",
            "3      0  Menurut Kepala Perwakilan BPKP di Provinsi Pap...\n",
            "4      0  Indonesia memiliki potensi yang luar biasa seb...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qy7RpAa5nnw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install -U nltk\n",
        "#import ntlk\n",
        "#ntlk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNLlddIv6eiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "train_df['noun_count'] = train_df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "train_df['verb_count'] = train_df['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "train_df['adj_count'] = train_df['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "train_df['adv_count'] = train_df['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "train_df['pron_count'] = train_df['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q32Bm7xp6eq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\"predictions\", predictions[:20])\n",
        "    print(\"valid_y\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e130b4b1-a551-466f-81c3-634988954305"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, Count Vectors:  0.9360279150915964\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, WordLevel TF-IDF:  0.9274498400697877\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 1 1 0 0 0 1 2 0 0]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, N-Gram Vectors:  0.7916545507414946\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "NB, CharLevel Vectors:  0.8685664437336436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "27fe4338-b48f-4f9b-bf45-6ed1644a02ed"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, Count Vectors:  0.9520209363186973\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, WordLevel TF-IDF:  0.9358825239895319\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 1 3 0 3 0 1 0 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, N-Gram Vectors:  0.8269845885431811\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "LR, CharLevel Vectors:  0.9017156150043617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "33655ac7-dfe7-4b04-e7a1-db85e40c1a30"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "SVM, N-Gram Vectors:  0.4287583599883687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "73992efd-2814-455d-a90e-d20a47f79e27"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 1 1 0 0 0 1 2 3 0 0 0 0 3 0 1 2 0 0]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, Count Vectors:  0.8895027624309392\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "RF, WordLevel TF-IDF:  0.8764175632451294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "c6ab3a96-9b58-4dfa-889e-66610658ece6"
      },
      "cell_type": "code",
      "source": [
        "# Extreme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 0]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, Count Vectors:  0.8200058156440826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 0 2 0 0]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, WordLevel TF-IDF:  0.8220412910729863\n",
            "predictions [1 0 1 1 0 0 0 1 2 3 0 2 2 0 3 0 1 2 0 1]\n",
            "valid_y [1 0 1 1 0 0 0 1 2 3 0 2 1 0 3 0 1 2 0 1]\n",
            "Xgb, CharLevel Vectors:  0.8298924105844723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        print(\">> Init text:\", self.texts[0])\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        #print(\">> on_epoch_end:\")\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0929c81f-4723-4145-b1bb-cefec5288ff7"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7500, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_tpu(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    print(\"TPU Wrapper start\")\n",
        "    print(func)\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    print(\"TPU Wrapper end\")\n",
        "    return tpu_model(*args, **kwargs)\n",
        "  return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOK2hrFuYnIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b60d3088-4b24-411f-9c26-c1a078598132"
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Init text: Pada petenis unggulan mendapatkan bye pada pertandingan yang berlangsung di Indian Wells Tennis Garden.\n",
            ">> Init text: Pasukan dari divisi infantri ke tiga yang bermarkas di Fort Stewart, Georgia, itu akan berangkat ke Irak pada Maret bukan pada bulan Juni, dan mereka akan membantu melakukan tugas intelejen, penjagaan dan pengendalian, kata Pentagon.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "5e379df4-1f2f-4731-dbe7-6fb59ddb1a25"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 4s 216ms/step - loss: 1.0385 - acc: 0.5819 - val_loss: 0.5606 - val_acc: 0.8563\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 3s 139ms/step - loss: 0.5031 - acc: 0.8357 - val_loss: 0.3341 - val_acc: 0.8890\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 2s 123ms/step - loss: 0.3664 - acc: 0.8794 - val_loss: 0.2829 - val_acc: 0.8981\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.3236 - acc: 0.8904 - val_loss: 0.2558 - val_acc: 0.9090\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.2885 - acc: 0.9036 - val_loss: 0.2376 - val_acc: 0.9178\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 3s 144ms/step - loss: 0.2641 - acc: 0.9115 - val_loss: 0.2260 - val_acc: 0.9204\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.2488 - acc: 0.9167 - val_loss: 0.2136 - val_acc: 0.9272\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.2340 - acc: 0.9217 - val_loss: 0.2006 - val_acc: 0.9274\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.2150 - acc: 0.9278 - val_loss: 0.1974 - val_acc: 0.9299\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 2s 125ms/step - loss: 0.2070 - acc: 0.9301 - val_loss: 0.1907 - val_acc: 0.9325\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 0.1918 - acc: 0.9336 - val_loss: 0.1857 - val_acc: 0.9341\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.1837 - acc: 0.9373 - val_loss: 0.1839 - val_acc: 0.9342\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 0.1730 - acc: 0.9419 - val_loss: 0.1784 - val_acc: 0.9365\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1640 - acc: 0.9450 - val_loss: 0.1687 - val_acc: 0.9414\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1590 - acc: 0.9458 - val_loss: 0.1658 - val_acc: 0.9422\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 3s 132ms/step - loss: 0.1466 - acc: 0.9507 - val_loss: 0.1618 - val_acc: 0.9445\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 0.1411 - acc: 0.9534 - val_loss: 0.1702 - val_acc: 0.9417\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 0.1265 - acc: 0.9578 - val_loss: 0.1683 - val_acc: 0.9409\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 136ms/step - loss: 0.1242 - acc: 0.9580 - val_loss: 0.1624 - val_acc: 0.9434\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 2s 120ms/step - loss: 0.1134 - acc: 0.9627 - val_loss: 0.1653 - val_acc: 0.9451\n",
            "CPU times: user 59.3 s, sys: 12.3 s, total: 1min 11s\n",
            "Wall time: 55.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b20d43a1-cbf6-4bce-be25-ebcbb08a9072"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on validation dataset\n",
        "\n",
        "\n",
        "sequences = token.texts_to_sequences(test_df['text'].values)\n",
        "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "\"\"\"\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\"\"\"\n",
        "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
        "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 0s 108us/step\n",
            "Loss for final step: 0.17674767505865135, accuracy: 0.9450261780104712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "diSp_8Sg9rrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "446700b5-a6cf-4835-d617-435fb776b3ff"
      },
      "cell_type": "code",
      "source": [
        "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
        "#print(labels[:5])\n",
        "print(np.array(labels)[:5])\n",
        "print(test_df['label'].values)\n",
        "print(to_categorical(3, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 0]\n",
            " [0 1 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]]\n",
            "[2 1 0 ... 2 0 2]\n",
            "[0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6c6b0e67-8984-4ca2-fff3-9365424e2bd7"
      },
      "cell_type": "code",
      "source": [
        "score = classifier.predict(test_x_seq, verbose=1)\n",
        "#print(score)\n",
        "print('Loss for final step: {}'.format(np.argmax(score, axis=1)[:10]))\n",
        "print(valid_x[:10])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3056/3056 [==============================] - 0s 77us/step\n",
            "Loss for final step: [2 1 0 0 0 2 2 0 2 0]\n",
            "7715     Pasukan dari divisi infantri ke tiga yang berm...\n",
            "12394    Pemerintah dan BI akan tetap menjaga stabilita...\n",
            "7327     Israel mulai melonggarkan pembatasan di pos pe...\n",
            "20168    Dua pria tewas dan dua orang lainnya cedera da...\n",
            "21502    Ia mengatakan meski saat ini banyak arus modal...\n",
            "19559    Agri Resources BV, sebuah perusahaan di Beland...\n",
            "6336     Asalkan kita punya program-program yang bagus ...\n",
            "2881     Sikap ini diperkuat oleh hasil interogasi terh...\n",
            "16877            Gurita juga menggunakan sistem yang sama.\n",
            "26868    Tantangan utamanya mungkin akan datang dari Ra...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ace56d9c-a28f-49c0-c191-0398366126b0"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335236     1705600     3564096         804     8065540    12425368\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7f976e14-3199-46b5-e78c-a4962e50c768"
      },
      "cell_type": "code",
      "source": [
        "print(score)\n",
        "print(np.argmax(score, axis=1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.3721889e-04 1.7559213e-04 9.9948490e-01 2.2181157e-06]\n",
            " [1.0414308e-08 9.9999595e-01 2.9743585e-06 1.0576000e-06]\n",
            " [9.7819394e-01 6.5141567e-03 1.5249715e-02 4.2119769e-05]\n",
            " ...\n",
            " [1.3407861e-02 1.9714341e-02 9.6600837e-01 8.6940167e-04]\n",
            " [9.9999952e-01 3.1176335e-09 4.6144635e-07 4.8664687e-11]\n",
            " [2.9894806e-04 7.0751207e-07 9.9969995e-01 4.3038932e-07]]\n",
            "[2 1 0 ... 2 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
        "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
        "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
        "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "76420a26-54a2-4953-d3d2-46de64817bb6"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 12s 608ms/step - loss: 1.0240 - acc: 0.5697 - val_loss: 0.5509 - val_acc: 0.8366\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 9s 439ms/step - loss: 0.4237 - acc: 0.8557 - val_loss: 0.2883 - val_acc: 0.8965\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.3061 - acc: 0.8934 - val_loss: 0.2479 - val_acc: 0.9118\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 9s 435ms/step - loss: 0.2696 - acc: 0.9048 - val_loss: 0.2386 - val_acc: 0.9149\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 9s 440ms/step - loss: 0.2481 - acc: 0.9127 - val_loss: 0.2233 - val_acc: 0.9219\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.2391 - acc: 0.9166 - val_loss: 0.2154 - val_acc: 0.9271\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 9s 446ms/step - loss: 0.2238 - acc: 0.9207 - val_loss: 0.2110 - val_acc: 0.9251\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 9s 444ms/step - loss: 0.2166 - acc: 0.9214 - val_loss: 0.2010 - val_acc: 0.9320\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.2109 - acc: 0.9268 - val_loss: 0.2027 - val_acc: 0.9310\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 9s 442ms/step - loss: 0.2061 - acc: 0.9259 - val_loss: 0.2138 - val_acc: 0.9250\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 9s 440ms/step - loss: 0.2039 - acc: 0.9271 - val_loss: 0.2068 - val_acc: 0.9274\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 9s 438ms/step - loss: 0.1989 - acc: 0.9286 - val_loss: 0.1972 - val_acc: 0.9323\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 9s 437ms/step - loss: 0.1866 - acc: 0.9334 - val_loss: 0.1880 - val_acc: 0.9357\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1755 - acc: 0.9358 - val_loss: 0.1929 - val_acc: 0.9323\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 9s 438ms/step - loss: 0.1746 - acc: 0.9360 - val_loss: 0.1844 - val_acc: 0.9372\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1666 - acc: 0.9403 - val_loss: 0.1861 - val_acc: 0.9336\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 9s 434ms/step - loss: 0.1637 - acc: 0.9408 - val_loss: 0.1871 - val_acc: 0.9377\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 9s 433ms/step - loss: 0.1554 - acc: 0.9452 - val_loss: 0.1800 - val_acc: 0.9395\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1530 - acc: 0.9457 - val_loss: 0.1796 - val_acc: 0.9398\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 9s 436ms/step - loss: 0.1440 - acc: 0.9497 - val_loss: 0.1773 - val_acc: 0.9408\n",
            "CPU times: user 4min 5s, sys: 25.3 s, total: 4min 30s\n",
            "Wall time: 3min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1204
        },
        "outputId": "923e65b2-4ff4-44fb-e84c-767c091fcb45"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5dc3ae600262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ndef create_rnn_gru():\\n    # Add an Input Layer\\n    input_layer = layers.Input((70, ))\\n\\n    # Add the word embedding Layer\\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\\n\\n    # Add the GRU Layer\\n    lstm_layer = layers.GRU(100)(embedding_layer)\\n\\n    # Add the output Layers\\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\\n\\n    # Compile the model\\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\\n    model.compile(optimizer=optimizers.Adam(), loss=\\'binary_crossentropy\\',\\n                  metrics=[\\'accuracy\\'])\\n    \\n    return model\\n\\nclassifier = create_rnn_gru()\\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \\n                       is_neural_net=True, epochs=10)\\nprint(\"RNN-GRU, Word Embeddings\",  accuracy)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-dfa01db8d1c0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 993\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    326\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have shape (70,) but got array with shape (100,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mM-MA8WKRg22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fbde5554-0f4d-4a9e-c3b3-7a2f0b3924e5"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 38s 5ms/step - loss: 0.6029\n",
            "predictions [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "RNN-Bidirectional, Word Embeddings 0.7936\n",
            "CPU times: user 1min 21s, sys: 1.16 s, total: 1min 22s\n",
            "Wall time: 45.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "cf50faff-2384-4456-c5b0-c70ef7f89f25"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7500/7500 [==============================] - 12s 2ms/step - loss: 0.5678\n",
            "Epoch 2/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3829\n",
            "Epoch 3/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3374\n",
            "Epoch 4/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2814\n",
            "Epoch 5/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2365\n",
            "Epoch 6/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2043\n",
            "Epoch 7/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1675\n",
            "Epoch 8/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1320\n",
            "Epoch 9/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1081\n",
            "Epoch 10/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1005\n",
            "predictions [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "CNN, Word Embeddings 0.8624\n",
            "CPU times: user 3min 22s, sys: 4.89 s, total: 3min 27s\n",
            "Wall time: 1min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}