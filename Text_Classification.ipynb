{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "037a2938-b534-49e7-b4a5-71b29f64ba4f"
      },
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/18/7f55c8be6d68ddc4036ffda5382ca51e23a1075987f708b9123712091af1/textblob-0.15.1-py2.py3-none-any.whl (631kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce84f6cf-0a13-4f9c-a19b-2cfe6b0ac03c"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas as pd, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ae84418d-aa49-4f86-c8e3-e17a7cdb717b"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o-a91KpeMmjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('amazon-review.csv')\n",
        "#drive/My Drive/lmdata/amazon-review.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plSjmVBqvEgf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "data = open('drive/My Drive/lmdata/amazon-review.csv').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split(\" \", 1)\n",
        "    labels.append(content[0])\n",
        "    texts.append(content[1])\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJnODG4RN6Lb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d3fc2f61-d5e5-466e-edfb-0bfe1068cdcd"
      },
      "cell_type": "code",
      "source": [
        "print(trainDF.head())\n",
        "print(texts[:2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text       label\n",
            "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
            "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
            "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
            "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
            "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
            "['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^', \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kIoriTXxxMFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f33f89f5-b845-45fc-c15a-0a6cdbfb8783"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -l \"/content/drive/My Drive/lmdata\"\n",
        "#!ls -l \"/content/\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 4402\n",
            "-rw------- 1 root root 4507148 Apr 21 23:28 amazon-review.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "58b5c971-411b-45d0-9238-24edd587c95c"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(valid_x[:5])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9434    plastic tool: works ok but not really enough c...\n",
            "8630    transliteration makes lao even more complex th...\n",
            "3940    This product should be recalled: Please don't ...\n",
            "880     a book value: The classic book that is read by...\n",
            "7481    Would have been nice except all the bugs!: Thi...\n",
            "Name: text, dtype: object\n",
            "5042    Great ballads.: Very nice playing, especially ...\n",
            "5274    A disappointment: Having heard the author spea...\n",
            "7683    Great: I bought the lapinator and Mousitizer a...\n",
            "1206    Power connector is not durable: Adapter worked...\n",
            "7303    Awful and boring: It was one of the most borin...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfzSkzK9OSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(xtrain_count, xvalid_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki-news-300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNxd48596LbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainDF['char_count'] = trainDF['text'].apply(len)\n",
        "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNLlddIv6eiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q32Bm7xp6eq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print \"NB, Count Vectors: \", accuracy\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print \"NB, WordLevel TF-IDF: \", accuracy\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print \"NB, N-Gram Vectors: \", accuracy\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print \"NB, CharLevel Vectors: \", accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print \"LR, Count Vectors: \", accuracy\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print \"LR, WordLevel TF-IDF: \", accuracy\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print \"LR, N-Gram Vectors: \", accuracy\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print \"LR, CharLevel Vectors: \", accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print \"SVM, N-Gram Vectors: \", accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print \"RF, Count Vectors: \", accuracy\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print \"RF, WordLevel TF-IDF: \", accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print \"Xgb, Count Vectors: \", accuracy\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print \"Xgb, WordLevel TF-IDF: \", accuracy\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print \"Xgb, CharLevel Vectors: \", accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
        "print \"NN, Ngram Level TF IDF Vectors\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqDvHa3n7Cj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print \"RNN-LSTM, Word Embeddings\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print \"RNN-GRU, Word Embeddings\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print \"RNN-Bidirectional, Word Embeddings\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print \"CNN, Word Embeddings\",  accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}