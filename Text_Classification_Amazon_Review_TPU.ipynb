{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification - Amazon Review - TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cahya-wirawan/ML-Collection/blob/master/Text_Classification_Amazon_Review_TPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8soltuwsMDp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "88dbc746-54c7-45ff-f561-9c46110da64a"
      },
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "#!pip install -U ntlk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/18/7f55c8be6d68ddc4036ffda5382ca51e23a1075987f708b9123712091af1/textblob-0.15.1-py2.py3-none-any.whl (631kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.11.0)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aUvM1cMBuWq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import tensorflow as tf\n",
        "#import pandas as pd, xgboost, numpy, textblob, string\n",
        "import pandas as pd, xgboost, numpy, string\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "#import ntlk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnhzNIbNTS63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "f8e3d168-bff8-467f-f4dc-d84e0c07650c"
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "LMDATA = Path('/content/drive/My Drive/lmdata')\n",
        "params = {'batch_size': 1024,\n",
        "          'n_classes': 2,\n",
        "          'max_len': 100,\n",
        "          'n_words': 50000,\n",
        "          'shuffle': True}\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1c523f1e6b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0;34m'n_words'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           'shuffle': True}\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mTPU_WORKER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m strategy=tf.contrib.tpu.TPUDistributionStrategy(\n\u001b[1;32m      9\u001b[0m         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
            "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Ga2f121_yrnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#for fn in uploaded.keys():\n",
        "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oIkg4wIONNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6ab01526-aa62-4ffa-a986-202480207d61"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMk8Dhu4gvv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ae2301cf-a752-4258-da69-c6f6dd55dc1a"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "print(LMDATA)\n",
        "!ls -lh \"$LMDATA\"\n",
        "#!ls -l \"/content/\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/lmdata\n",
            "total 2.3G\n",
            "-rw------- 1 root root 4.3M Apr 21 23:28 amazon-review.csv\n",
            "-rw------- 1 root root  58M Oct  8 16:03 bard-1.h5\n",
            "-rw------- 1 root root  39M Oct  9 10:32 bard.h5\n",
            "-rw------- 1 root root  22M Oct  8 13:31 wiki-news-300d-10K.vec\n",
            "-rw------- 1 root root 2.2G Mar 14  2018 wiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o-a91KpeMmjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('amazon-review.csv')\n",
        "#drive/My Drive/lmdata/amazon-review.csv\n",
        "#LMDATA = \"/content/drive/My Drive/lmdata\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plSjmVBqvEgf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# load the dataset\n",
        "data = open(LMDATA/'amazon-review.csv').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split(\" \", 1)\n",
        "    labels.append(content[0])\n",
        "    texts.append(content[1])\n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJnODG4RN6Lb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "2bb047ea-c98f-484c-e0dc-f2b11a955678"
      },
      "cell_type": "code",
      "source": [
        "print(trainDF.head())\n",
        "print(texts[:2])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text       label\n",
            "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
            "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
            "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
            "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
            "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
            "['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^', \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z5vrNcqhRyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fd80d112-57a2-4bae-d717-e8c7decd3b55"
      },
      "cell_type": "code",
      "source": [
        "#!pip show tensorflow\n",
        "#!nvidia-smi\n",
        "!set |grep -i tpu|grep -v grep"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COLAB_TPU_ADDR=10.20.112.162:8470\n",
            "DATALAB_SETTINGS_OVERRIDES='{\"datalabBasePath\":\"/tun/m/tpu-2rjvnxs4v9jri/\",\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"notebook\",\"-y\",\"--no-browser\",\"--log-level=DEBUG\",\"--debug\",\"--NotebookApp.allow_origin=\\\"*\\\"\",\"--NotebookApp.log_format=\\\"%(message)s\\\"\",\"--NotebookApp.disable_check_xsrf=True\",\"--NotebookApp.token=\",\"--Session.key=\\\"\\\"\",\"--Session.keyfile=\\\"\\\"\",\"--ContentsManager.untitled_directory=\\\"Untitled Folder\\\"\",\"--ContentsManager.untitled_file=\\\"Untitled File\\\"\",\"--ContentsManager.untitled_notebook=\\\"Untitled Notebook\\\"\",\"--KernelManager.autorestart=True\",\"--ip=\\\"172.28.0.2\\\"\"]}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nf8wMdASa-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9LwD3V_TOJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ce78022e-e793-4f44-f519-e73e8ae2dc39"
      },
      "cell_type": "code",
      "source": [
        "print(train_x[:5])\n",
        "print(train_y[:5])\n",
        "print(valid_x[:5])\n",
        "print(valid_y[:5])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4801    Not so great: This is NOT Koss at its best. I ...\n",
            "3163    SLOW very very slow: Know this the web site fo...\n",
            "4222    Seems to be good!: This seems to be good so fa...\n",
            "4945    Book by Dale Carnegie: I read book Quick and E...\n",
            "2677    Disappointing: This poor character never catch...\n",
            "Name: text, dtype: object\n",
            "[0 0 1 1 0]\n",
            "6204    You will FALL IN LOVE with Auel's Ayla!!: You ...\n",
            "6165    Looks like it was Printed on a cheap broken do...\n",
            "5509    I used to eat it till I found out how it was m...\n",
            "2465    I made better recordings in my garage: I was e...\n",
            "9348    One of the all time best reads ever!: I read t...\n",
            "Name: text, dtype: object\n",
            "[1 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMIMaiYGSvaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfzSkzK9OSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(xtrain_count, xvalid_count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dS7tm2i5e32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                             max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
        "                                   ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
        "                                         ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YErL4IVA5jrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6796e317-74a3-4ce1-edf6-d5b2765b30dd"
      },
      "cell_type": "code",
      "source": [
        "# load the pre-trained word-embedding vectors\n",
        "max_words = params['n_words']\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('drive/My Drive/lmdata/wiki-news-300d-10K.vec')):\n",
        "  if i%1000 == 0:\n",
        "    print(i)\n",
        "  values = line.split()\n",
        "  embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_words:\n",
        "        break\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JNxd48596LbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainDF['char_count'] = trainDF['text'].apply(len)\n",
        "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrGptFBdhuYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "99c78a9a-9af1-45ce-b283-81fccdb1fda4"
      },
      "cell_type": "code",
      "source": [
        "print(trainDF.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text       label\n",
            "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
            "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
            "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
            "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
            "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qy7RpAa5nnw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install -U nltk\n",
        "#import ntlk\n",
        "#ntlk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNLlddIv6eiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q32Bm7xp6eq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AFNBPZq535al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
        "                is_neural_net=False, epochs=1):\n",
        "    # fit the training dataset on the classifier\n",
        "    if is_neural_net:\n",
        "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
        "    else:\n",
        "      classifier.fit(feature_vector_train, label)   \n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    \n",
        "    if is_neural_net:\n",
        "      predictions = [int(round(p[0])) for p in predictions]\n",
        "      #predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    print(\"predictions\", predictions[:20])\n",
        "    print(\"valid_y\", valid_y[:20])\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE4iaE0X6ezd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8d4b286c-6281-4b8d-e529-7d7f9e7addbc"
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "NB, Count Vectors:  0.838\n",
            "predictions [1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "NB, WordLevel TF-IDF:  0.8508\n",
            "predictions [1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "NB, N-Gram Vectors:  0.8352\n",
            "predictions [1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "NB, CharLevel Vectors:  0.8052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWRC2U6N63iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "58183b5b-9d36-48d9-8530-23542dac7d2d"
      },
      "cell_type": "code",
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print( \"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "LR, Count Vectors:  0.8648\n",
            "predictions [1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "LR, WordLevel TF-IDF:  0.8728\n",
            "predictions [1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "LR, N-Gram Vectors:  0.8308\n",
            "predictions [1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1]\n",
            "valid_y [1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1]\n",
            "LR, CharLevel Vectors:  0.8356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcZq7jDH63v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5882823f-3f90-4d6e-c849-59c6519ad874"
      },
      "cell_type": "code",
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "SVM, N-Gram Vectors:  0.5152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGuz7Mk7634V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f9ab597a-225c-4c4a-ac31-76cb81ddcf6a"
      },
      "cell_type": "code",
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions [0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "RF, Count Vectors:  0.744\n",
            "predictions [0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "RF, WordLevel TF-IDF:  0.766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXX0OcYi63-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "5358645b-734d-4761-f62c-b7f276bebd8c"
      },
      "cell_type": "code",
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predictions [0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "Xgb, Count Vectors:  0.8052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predictions [0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "Xgb, WordLevel TF-IDF:  0.8032\n",
            "predictions [0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "Xgb, CharLevel Vectors:  0.806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JRsda_s3E-a2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2BE61kS1cYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "def tokenize(texts, n_words=1000):\n",
        "    tokenizer = Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "  \n",
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
        "                 n_classes=2, n_words=1000, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        print(\">> Init text:\", self.texts[0])\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        texts = np.array([self.texts[k] for k in indexes])\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "        y = np.array([self.labels[k] for k in indexes])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        #print(\">> on_epoch_end:\")\n",
        "        self.indexes = np.arange(self.texts.size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRkl0XIh7CZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-3Rizvqf9N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0929c81f-4723-4145-b1bb-cefec5288ff7"
      },
      "cell_type": "code",
      "source": [
        "xtrain_tfidf_ngram.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7500, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "ReP47N1IJ2qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_tpu(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    print(\"TPU Wrapper start\")\n",
        "    print(func)\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
        "    print(\"TPU Wrapper end\")\n",
        "    return tpu_model(*args, **kwargs)\n",
        "  return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRB-JGti7CfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_cnn(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
        "                                       weights=[embedding_matrix], \n",
        "                                       trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOK2hrFuYnIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "71b6645b-de16-4105-fc1f-13c7ae34f2f6"
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "print(\"training_generator:{}, {}\".format(type(train_x), train_y))\n",
        "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
        "print(\"valid_generator:{}\".format(type(valid_x)))\n",
        "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)\n",
        "print(\"read generator\")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_generator:<class 'pandas.core.series.Series'>, [1 1 0 ... 1 1 1]\n",
            ">> Init text: Fantastic. Up close and personal. Scripturally accurate.: Doesn't get much better than this. What a blessing I have recieved from Pink's expositional approach to this great King's life.\n",
            "valid_generator:<class 'pandas.core.series.Series'>\n",
            ">> Init text: So Far So Good!: I've only worn this for a day so far, but I love it! I still have a little bit of a tummy from the birth of my son 8 months ago. This really holds you in, its comfortable and I don't think it shows through your clothes! I highly recommend!!\n",
            "read generator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9E2diw6edp0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "b78793e2-0d69-4d3a-8eea-a4eeae9d7cc6"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 3s 360ms/step - loss: 0.6963 - acc: 0.5166 - val_loss: 0.6824 - val_acc: 0.6504\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 1s 180ms/step - loss: 0.6799 - acc: 0.5818 - val_loss: 0.6656 - val_acc: 0.7285\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 1s 137ms/step - loss: 0.6603 - acc: 0.6394 - val_loss: 0.6413 - val_acc: 0.7495\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 1s 158ms/step - loss: 0.6351 - acc: 0.6921 - val_loss: 0.6061 - val_acc: 0.7710\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 1s 158ms/step - loss: 0.5980 - acc: 0.7303 - val_loss: 0.5571 - val_acc: 0.7847\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.5501 - acc: 0.7592 - val_loss: 0.5031 - val_acc: 0.7915\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 1s 160ms/step - loss: 0.4998 - acc: 0.7881 - val_loss: 0.4583 - val_acc: 0.8032\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 1s 143ms/step - loss: 0.4557 - acc: 0.8078 - val_loss: 0.4254 - val_acc: 0.8076\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 1s 167ms/step - loss: 0.4227 - acc: 0.8104 - val_loss: 0.4090 - val_acc: 0.8135\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 1s 173ms/step - loss: 0.4039 - acc: 0.8221 - val_loss: 0.3921 - val_acc: 0.8149\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 0.3881 - acc: 0.8302 - val_loss: 0.3716 - val_acc: 0.8350\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 1s 195ms/step - loss: 0.3671 - acc: 0.8456 - val_loss: 0.3616 - val_acc: 0.8408\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 0.3596 - acc: 0.8431 - val_loss: 0.3534 - val_acc: 0.8403\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 1s 166ms/step - loss: 0.3364 - acc: 0.8592 - val_loss: 0.3463 - val_acc: 0.8452\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 1s 146ms/step - loss: 0.3305 - acc: 0.8638 - val_loss: 0.3403 - val_acc: 0.8511\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 1s 157ms/step - loss: 0.3199 - acc: 0.8676 - val_loss: 0.3332 - val_acc: 0.8530\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 0.3094 - acc: 0.8707 - val_loss: 0.3284 - val_acc: 0.8555\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 1s 159ms/step - loss: 0.3006 - acc: 0.8764 - val_loss: 0.3263 - val_acc: 0.8540\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 1s 151ms/step - loss: 0.2873 - acc: 0.8856 - val_loss: 0.3205 - val_acc: 0.8560\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 0.2819 - acc: 0.8883 - val_loss: 0.3145 - val_acc: 0.8569\n",
            "CPU times: user 25.7 s, sys: 2.91 s, total: 28.6 s\n",
            "Wall time: 25.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAQacrW3B3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VN_Nn0bxC0lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e375e65c-820a-49f1-c46f-a7886dc0b6a4"
      },
      "cell_type": "code",
      "source": [
        "# predict the labels on validation dataset\n",
        "\n",
        "\n",
        "sequences = token.texts_to_sequences(valid_x.values)\n",
        "valid_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
        "#print(valid_x_seq[:5])\n",
        "classifier = create_cnn(input_length=params['max_len'])\n",
        "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
        "\"\"\"\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\"\"\"\n",
        "score = classifier.evaluate(valid_x_seq, valid_y, verbose=1)\n",
        "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500/2500 [==============================] - 1s 248us/step\n",
            "Loss for final step: 0.3150304111480713, accuracy: 0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ExtDobFzkPmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d4989fe9-a00d-4f04-d77d-0b5d1b06a99b"
      },
      "cell_type": "code",
      "source": [
        "score = classifier.predict(valid_x_seq, verbose=1)\n",
        "print('Loss for final step: {}'.format(np.round(score).astype(int).reshape(-1)[:5]))\n",
        "print(valid_x[:5])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500/2500 [==============================] - 0s 93us/step\n",
            "Loss for final step: [1 1 1 1 1]\n",
            "642     So Far So Good!: I've only worn this for a day...\n",
            "8690    GOOD LOOKIN'!!: My girlfriend told me I had to...\n",
            "6335    Great Game.... :): This is a game everyone sho...\n",
            "6605    Great Value Priced CD: Saw a TV Show not long ...\n",
            "515     Back to basics...: Sometimes back to basics is...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70o-IXxgincl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2c5baf13-5e19-4ad2-b22f-2edc26692daf"
      },
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13335236     1136812     8625904         804     3572520    12333120\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZjhWhQdWogw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#print('Accuracy ', score[1])\n",
        "\"\"\"\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "#prediction_model.reset_states()\n",
        "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    prediction_model, strategy=strategy)\n",
        "\n",
        "predictions = prediction_model.predict(valid_x_seq)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "\n",
        "predictions = tpu_model.predict(valid_x_seq)\n",
        "\n",
        "predictions = [int(round(p[0])) for p in predictions]\n",
        "#predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"predictions\", predictions[:20])\n",
        "print(\"valid_y\", valid_y[:20])\n",
        "\n",
        "return metrics.accuracy_score(predictions, valid_y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaL1Vz7AU9A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d3cd1c33-bea4-4d41-a5e3-a18cc9a24bd5"
      },
      "cell_type": "code",
      "source": [
        "print(score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.07394564]\n",
            " [0.96206063]\n",
            " [0.9657972 ]\n",
            " ...\n",
            " [0.00592667]\n",
            " [0.35479817]\n",
            " [0.11346152]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkwx3dnI-DxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "CCERYjHmY-o6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_rnn_lstm(input_length=100):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_length, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0q7TZkZQaN5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1006
        },
        "outputId": "0561d17d-a599-470f-fbe6-9ae6ac2dad33"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
        "classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
        "classifier.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=valid_generator,\n",
        "    #use_multiprocessing=True,\n",
        "    #workers=6,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Get updates: Tensor(\"loss_12/mul:0\", shape=(), dtype=float32)\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.8834638595581055 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "6/7 [========================>.....] - ETA: 2s - loss: 0.6912 - acc: 0.5078INFO:tensorflow:New input shapes; (re-)compiling: mode=eval, [TensorSpec(shape=(128, 100), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_1_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.4813127517700195 secs\n",
            "7/7 [==============================] - 23s 3s/step - loss: 0.6908 - acc: 0.5212 - val_loss: 0.6857 - val_acc: 0.5703\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 1s 213ms/step - loss: 0.6766 - acc: 0.6071 - val_loss: 0.6696 - val_acc: 0.6328\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 2s 226ms/step - loss: 0.6439 - acc: 0.6451 - val_loss: 0.6081 - val_acc: 0.6836\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 2s 219ms/step - loss: 0.6018 - acc: 0.6886 - val_loss: 0.5708 - val_acc: 0.7031\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 1s 213ms/step - loss: 0.5349 - acc: 0.7533 - val_loss: 0.4592 - val_acc: 0.8281\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 2s 224ms/step - loss: 0.5067 - acc: 0.7522 - val_loss: 0.4367 - val_acc: 0.8320\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 1s 208ms/step - loss: 0.4990 - acc: 0.7712 - val_loss: 0.4361 - val_acc: 0.8008\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 1s 212ms/step - loss: 0.4674 - acc: 0.7857 - val_loss: 0.4209 - val_acc: 0.8203\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 1s 195ms/step - loss: 0.4784 - acc: 0.7768 - val_loss: 0.4130 - val_acc: 0.8359\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 1s 208ms/step - loss: 0.4636 - acc: 0.7757 - val_loss: 0.3871 - val_acc: 0.8516\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 1s 203ms/step - loss: 0.4867 - acc: 0.7690 - val_loss: 0.3129 - val_acc: 0.8828\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 1s 204ms/step - loss: 0.4288 - acc: 0.8069 - val_loss: 0.4181 - val_acc: 0.8125\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 1s 209ms/step - loss: 0.4428 - acc: 0.8125 - val_loss: 0.3439 - val_acc: 0.8516\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 1s 195ms/step - loss: 0.4326 - acc: 0.7969 - val_loss: 0.4059 - val_acc: 0.8281\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 2s 219ms/step - loss: 0.4318 - acc: 0.8114 - val_loss: 0.4151 - val_acc: 0.8008\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 0.4277 - acc: 0.8058 - val_loss: 0.4102 - val_acc: 0.8203\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 0.4375 - acc: 0.7958 - val_loss: 0.3509 - val_acc: 0.8398\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 2s 219ms/step - loss: 0.4326 - acc: 0.7991 - val_loss: 0.3661 - val_acc: 0.8672\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 1s 201ms/step - loss: 0.4178 - acc: 0.8170 - val_loss: 0.4377 - val_acc: 0.7930\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 0.4373 - acc: 0.7980 - val_loss: 0.4282 - val_acc: 0.8164\n",
            "CPU times: user 22 s, sys: 1.64 s, total: 23.6 s\n",
            "Wall time: 52.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUaVdVIa7Cqc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1238
        },
        "outputId": "a52ebcbb-851e-4809-a099-7c3f07617060"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4128/7500 [===============>..............] - ETA: 19s - loss: 0.6575"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-688834eedb2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ndef create_rnn_gru():\\n    # Add an Input Layer\\n    input_layer = layers.Input((70, ))\\n\\n    # Add the word embedding Layer\\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\\n\\n    # Add the GRU Layer\\n    lstm_layer = layers.GRU(100)(embedding_layer)\\n\\n    # Add the output Layers\\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\\n\\n    # Compile the model\\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\\n    model.compile(optimizer=optimizers.Adam(), loss=\\'binary_crossentropy\\')\\n    \\n    return model\\n\\nclassifier = create_rnn_gru()\\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \\n                       is_neural_net=True, epochs=10)\\nprint(\"RNN-GRU, Word Embeddings\",  accuracy)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-dfa01db8d1c0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2978\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2980\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mM-MA8WKRg22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzPoMIX07Cyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fbde5554-0f4d-4a9e-c3b3-7a2f0b3924e5"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=1)\n",
        "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 38s 5ms/step - loss: 0.6029\n",
            "predictions [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "RNN-Bidirectional, Word Embeddings 0.7936\n",
            "CPU times: user 1min 21s, sys: 1.16 s, total: 1min 22s\n",
            "Wall time: 45.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-W8lnbJu7owp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "cf50faff-2384-4456-c5b0-c70ef7f89f25"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, \n",
        "                       is_neural_net=True, epochs=10)\n",
        "print(\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7500/7500 [==============================] - 12s 2ms/step - loss: 0.5678\n",
            "Epoch 2/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3829\n",
            "Epoch 3/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.3374\n",
            "Epoch 4/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2814\n",
            "Epoch 5/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2365\n",
            "Epoch 6/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.2043\n",
            "Epoch 7/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1675\n",
            "Epoch 8/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1320\n",
            "Epoch 9/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1081\n",
            "Epoch 10/10\n",
            "7500/7500 [==============================] - 11s 1ms/step - loss: 0.1005\n",
            "predictions [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
            "valid_y [0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0]\n",
            "CNN, Word Embeddings 0.8624\n",
            "CPU times: user 3min 22s, sys: 4.89 s, total: 3min 27s\n",
            "Wall time: 1min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uyhGXYp7qBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPnFQwbLRktt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}